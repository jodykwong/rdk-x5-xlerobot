#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Story 1.4: è¿ç»­è¯­éŸ³è¯†åˆ« - æ€§èƒ½ç›‘æ§
Performance Monitoring for Continuous Speech Recognition

å®æ—¶ç›‘æ§å’Œè‡ªåŠ¨è°ƒä¼˜ç³»ç»Ÿï¼Œç¡®ä¿30åˆ†é’Ÿè¿ç»­è¿è¡Œçš„ç¨³å®šæ€§ã€‚
ç›‘æ§æŒ‡æ ‡:
- CPUä½¿ç”¨ç‡
- å†…å­˜ä½¿ç”¨é‡
- è¯†åˆ«å»¶è¿Ÿ
- ååé‡
- è¯†åˆ«å‡†ç¡®ç‡
- å”¤é†’è¯æ£€æµ‹æˆåŠŸç‡

ä½œè€…: Dev Agent
æ•…äº‹ID: Story 1.4
"""

import time
import threading
import psutil
import statistics
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass, field
from enum import Enum, auto
from queue import Queue, Empty
import logging
import json
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹æšä¸¾"""
    CPU_USAGE = auto()          # CPUä½¿ç”¨ç‡ (0-100%)
    MEMORY_USAGE = auto()       # å†…å­˜ä½¿ç”¨é‡ (MB)
    LATENCY = auto()            # å»¶è¿Ÿ (ms)
    THROUGHPUT = auto()         # ååé‡ (samples/s)
    ACCURACY = auto()           # è¯†åˆ«å‡†ç¡®ç‡ (0-1)
    WAKE_WORD_SUCCESS = auto()  # å”¤é†’è¯æ£€æµ‹æˆåŠŸç‡ (0-1)
    POWER_CONSUMPTION = auto()  # åŠŸè€— (W)
    TEMPERATURE = auto()        # æ¸©åº¦ (Â°C)


class AlertLevel(Enum):
    """å‘Šè­¦çº§åˆ«æšä¸¾"""
    INFO = auto()
    WARNING = auto()
    CRITICAL = auto()
    EMERGENCY = auto()


@dataclass
class MetricPoint:
    """æŒ‡æ ‡æ•°æ®ç‚¹"""
    timestamp: float
    value: float
    metric_type: MetricType
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Alert:
    """å‘Šè­¦ä¿¡æ¯"""
    level: AlertLevel
    message: str
    timestamp: float
    metric_type: MetricType
    value: float
    threshold: float
    resolved: bool = False
    resolved_at: Optional[float] = None


class PerformanceMonitor:
    """
    æ€§èƒ½ç›‘æ§å™¨

    åŠŸèƒ½ç‰¹æ€§:
    - å®æ—¶é‡‡é›†æ€§èƒ½æŒ‡æ ‡
    - è‡ªåŠ¨è°ƒä¼˜ç®—æ³•
    - æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ
    - å†å²æ•°æ®åˆ†æ
    - æ€§èƒ½è¶‹åŠ¿é¢„æµ‹

    ç›‘æ§æŒ‡æ ‡:
    - CPUä½¿ç”¨ç‡ (<80%)
    - å†…å­˜ä½¿ç”¨é‡ (<300MB)
    - è¯†åˆ«å»¶è¿Ÿ (<300ms P95)
    - ååé‡ (>10 samples/s)
    - è¯†åˆ«å‡†ç¡®ç‡ (>90%)
    - å”¤é†’è¯æ£€æµ‹æˆåŠŸç‡ (>95%)
    """

    def __init__(self,
                 sample_interval: float = 1.0,
                 history_size: int = 1000,
                 enable_auto_tuning: bool = True):
        """
        åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨

        Args:
            sample_interval: é‡‡æ ·é—´éš” (ç§’)
            history_size: å†å²æ•°æ®å¤§å°
            enable_auto_tuning: æ˜¯å¦å¯ç”¨è‡ªåŠ¨è°ƒä¼˜
        """
        self.sample_interval = sample_interval
        self.history_size = history_size
        self.enable_auto_tuning = enable_auto_tuning

        # æŒ‡æ ‡å†å²æ•°æ®
        self._metrics_history: Dict[MetricType, List[MetricPoint]] = {
            metric_type: [] for metric_type in MetricType
        }
        self._metrics_lock = threading.RLock()

        # å‘Šè­¦ç®¡ç†
        self._alerts: List[Alert] = []
        self._alert_callbacks: List[Callable[[Alert], None]] = []

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            'total_samples': 0,
            'start_time': time.time(),
            'last_sample_time': 0.0,
            'auto_tuning_count': 0,
            'alert_count': 0,
            'resolved_alerts': 0
        }

        # ç³»ç»ŸåŸºçº¿
        self._baseline = {
            MetricType.CPU_USAGE: 0.0,
            MetricType.MEMORY_USAGE: 0.0,
            MetricType.LATENCY: 0.0,
            MetricType.THROUGHPUT: 0.0,
            MetricType.ACCURACY: 0.0,
            MetricType.WAKE_WORD_SUCCESS: 0.0
        }

        # è‡ªåŠ¨è°ƒä¼˜è§„åˆ™
        self._tuning_rules = {
            MetricType.CPU_USAGE: {
                'warning_threshold': 80.0,
                'critical_threshold': 90.0,
                'auto_action': self._tune_cpu_usage
            },
            MetricType.MEMORY_USAGE: {
                'warning_threshold': 250.0,  # MB
                'critical_threshold': 300.0,  # MB
                'auto_action': self._tune_memory_usage
            },
            MetricType.LATENCY: {
                'warning_threshold': 200.0,  # ms
                'critical_threshold': 300.0,  # ms
                'auto_action': self._tune_latency
            },
            MetricType.ACCURACY: {
                'warning_threshold': 0.85,
                'critical_threshold': 0.80,
                'auto_action': self._tune_accuracy
            },
            MetricType.WAKE_WORD_SUCCESS: {
                'warning_threshold': 0.90,
                'critical_threshold': 0.85,
                'auto_action': self._tune_wake_word
            }
        }

        # ç›‘æ§çº¿ç¨‹
        self._monitoring_active = False
        self._monitoring_thread: Optional[threading.Thread] = None

        # å¯åŠ¨ç›‘æ§
        self.start_monitoring()

        # å»ºç«‹åŸºçº¿
        self._establish_baseline()

        logger.info(f"æ€§èƒ½ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ: é‡‡æ ·é—´éš”={sample_interval}ç§’, "
                   f"è‡ªåŠ¨è°ƒä¼˜={'å¯ç”¨' if enable_auto_tuning else 'ç¦ç”¨'}")

    def start_monitoring(self) -> None:
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        if self._monitoring_active:
            logger.warning("æ€§èƒ½ç›‘æ§å·²åœ¨è¿è¡Œ")
            return

        self._monitoring_active = True
        self._monitoring_thread = threading.Thread(
            target=self._monitoring_worker,
            daemon=True
        )
        self._monitoring_thread.start()

        logger.info("ğŸ“Š æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")

    def stop_monitoring(self) -> None:
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        if not self._monitoring_active:
            return

        self._monitoring_active = False
        if self._monitoring_thread and self._monitoring_thread.is_alive():
            self._monitoring_thread.join(timeout=2.0)

        logger.info("ğŸ“Š æ€§èƒ½ç›‘æ§å·²åœæ­¢")

    def _monitoring_worker(self) -> None:
        """ç›‘æ§å·¥ä½œçº¿ç¨‹"""
        while self._monitoring_active:
            try:
                # é‡‡é›†ç³»ç»ŸæŒ‡æ ‡
                self._collect_system_metrics()

                # é‡‡é›†åº”ç”¨æŒ‡æ ‡
                self._collect_application_metrics()

                # æ£€æŸ¥å‘Šè­¦
                self._check_alerts()

                # æ‰§è¡Œè‡ªåŠ¨è°ƒä¼˜
                if self.enable_auto_tuning:
                    self._auto_tune()

                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                with self._metrics_lock:
                    self._stats['total_samples'] += 1
                    self._stats['last_sample_time'] = time.time()

                # ç­‰å¾…é‡‡æ ·é—´éš”
                time.sleep(self.sample_interval)

            except Exception as e:
                logger.error(f"ç›‘æ§å·¥ä½œçº¿ç¨‹é”™è¯¯: {e}")
                if self._monitoring_active:
                    time.sleep(1)

    def _collect_system_metrics(self) -> None:
        """é‡‡é›†ç³»ç»ŸæŒ‡æ ‡"""
        try:
            current_time = time.time()

            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=0.1)
            self._record_metric(MetricType.CPU_USAGE, cpu_percent, current_time)

            # å†…å­˜ä½¿ç”¨é‡
            memory = psutil.virtual_memory()
            memory_mb = (memory.total - memory.available) / 1024 / 1024
            self._record_metric(MetricType.MEMORY_USAGE, memory_mb, current_time)

        except Exception as e:
            logger.error(f"ç³»ç»ŸæŒ‡æ ‡é‡‡é›†å¤±è´¥: {e}")

    def _collect_application_metrics(self) -> None:
        """é‡‡é›†åº”ç”¨æŒ‡æ ‡ (éœ€è¦å¤–éƒ¨æ•°æ®æº)"""
        # åº”ç”¨æŒ‡æ ‡é€šå¸¸æ¥è‡ªå¤–éƒ¨æ•°æ®æºï¼Œå¦‚:
        # - ASRå¼•æ“çš„è¯†åˆ«å»¶è¿Ÿ
        # - è¯†åˆ«å‡†ç¡®ç‡
        # - å”¤é†’è¯æ£€æµ‹æˆåŠŸç‡
        # è¿™äº›æŒ‡æ ‡éœ€è¦é€šè¿‡ record_metric æ–¹æ³•æ‰‹åŠ¨è®°å½•

        pass

    def _record_metric(self,
                      metric_type: MetricType,
                      value: float,
                      timestamp: Optional[float] = None,
                      metadata: Optional[Dict[str, Any]] = None) -> None:
        """
        è®°å½•æŒ‡æ ‡æ•°æ®ç‚¹

        Args:
            metric_type: æŒ‡æ ‡ç±»å‹
            value: æŒ‡æ ‡å€¼
            timestamp: æ—¶é—´æˆ³ (å¯é€‰ï¼Œé»˜è®¤å½“å‰æ—¶é—´)
            metadata: å…ƒæ•°æ®
        """
        if timestamp is None:
            timestamp = time.time()

        # åˆ›å»ºæŒ‡æ ‡æ•°æ®ç‚¹
        point = MetricPoint(
            timestamp=timestamp,
            value=value,
            metric_type=metric_type,
            metadata=metadata or {}
        )

        # æ·»åŠ åˆ°å†å²æ•°æ®
        with self._metrics_lock:
            self._metrics_history[metric_type].append(point)

            # ä¿æŒå†å²æ•°æ®å¤§å°
            if len(self._metrics_history[metric_type]) > self.history_size:
                self._metrics_history[metric_type].pop(0)

    def get_metric_stats(self,
                        metric_type: MetricType,
                        duration_seconds: Optional[float] = None) -> Dict[str, float]:
        """
        è·å–æŒ‡æ ‡ç»Ÿè®¡ä¿¡æ¯

        Args:
            metric_type: æŒ‡æ ‡ç±»å‹
            duration_seconds: æ—¶é—´èŒƒå›´ (ç§’)ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨å†å²

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        with self._metrics_lock:
            points = self._metrics_history[metric_type]

            if not points:
                return {
                    'count': 0,
                    'min': 0.0,
                    'max': 0.0,
                    'mean': 0.0,
                    'median': 0.0,
                    'p95': 0.0,
                    'p99': 0.0
                }

            # è¿‡æ»¤æ—¶é—´èŒƒå›´
            if duration_seconds:
                cutoff_time = time.time() - duration_seconds
                points = [p for p in points if p.timestamp >= cutoff_time]

            # è®¡ç®—ç»Ÿè®¡å€¼
            values = [p.value for p in points]
            values.sort()

            count = len(values)
            minimum = values[0]
            maximum = values[-1]
            mean = statistics.mean(values)
            median = statistics.median(values)

            # è®¡ç®—ç™¾åˆ†ä½æ•°
            p95_index = int(count * 0.95)
            p99_index = int(count * 0.99)
            p95 = values[min(p95_index, count - 1)]
            p99 = values[min(p99_index, count - 1)]

            return {
                'count': count,
                'min': minimum,
                'max': maximum,
                'mean': mean,
                'median': median,
                'p95': p95,
                'p99': p99
            }

    def _check_alerts(self) -> None:
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        current_time = time.time()

        for metric_type, rule in self._tuning_rules.items():
            stats = self.get_metric_stats(metric_type, duration_seconds=60)  # æœ€è¿‘1åˆ†é’Ÿ

            if stats['count'] == 0:
                continue

            latest_value = self._get_latest_value(metric_type)

            # æ£€æŸ¥å‘Šè­¦é˜ˆå€¼
            warning_threshold = rule['warning_threshold']
            critical_threshold = rule['critical_threshold']

            # å¯¹äº"è¶Šä½è¶Šå¥½"çš„æŒ‡æ ‡ (å¦‚å»¶è¿Ÿ)
            if metric_type in [MetricType.LATENCY, MetricType.CPU_USAGE, MetricType.MEMORY_USAGE]:
                alert_level = None
                threshold = 0.0

                if latest_value >= critical_threshold:
                    alert_level = AlertLevel.CRITICAL
                    threshold = critical_threshold
                elif latest_value >= warning_threshold:
                    alert_level = AlertLevel.WARNING
                    threshold = warning_threshold

                # å¯¹äº"è¶Šé«˜è¶Šå¥½"çš„æŒ‡æ ‡ (å¦‚å‡†ç¡®ç‡)
            else:
                alert_level = None
                threshold = 0.0

                if latest_value <= critical_threshold:
                    alert_level = AlertLevel.CRITICAL
                    threshold = critical_threshold
                elif latest_value <= warning_threshold:
                    alert_level = AlertLevel.WARNING
                    threshold = warning_threshold

            # è§¦å‘å‘Šè­¦
            if alert_level:
                self._trigger_alert(alert_level, metric_type, latest_value, threshold)

    def _trigger_alert(self,
                      level: AlertLevel,
                      metric_type: MetricType,
                      value: float,
                      threshold: float) -> None:
        """è§¦å‘å‘Šè­¦"""
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æœªè§£å†³çš„ç›¸åŒå‘Šè­¦
        for alert in self._alerts:
            if (not alert.resolved and
                alert.metric_type == metric_type and
                alert.level == level):
                return  # å·²æœ‰ç›¸åŒå‘Šè­¦ï¼Œä¸é‡å¤è§¦å‘

        # åˆ›å»ºæ–°å‘Šè­¦
        alert = Alert(
            level=level,
            message=f"{metric_type.name} è¶…è¿‡é˜ˆå€¼: {value:.2f} > {threshold:.2f}",
            timestamp=time.time(),
            metric_type=metric_type,
            value=value,
            threshold=threshold
        )

        # æ·»åŠ åˆ°å‘Šè­¦åˆ—è¡¨
        self._alerts.append(alert)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        with self._metrics_lock:
            self._stats['alert_count'] += 1

        # è®°å½•æ—¥å¿—
        log_level = {
            AlertLevel.INFO: logging.INFO,
            AlertLevel.WARNING: logging.WARNING,
            AlertLevel.CRITICAL: logging.CRITICAL,
            AlertLevel.EMERGENCY: logging.CRITICAL
        }[level]

        logger.log(log_level, f"ğŸš¨ å‘Šè­¦: {alert.message}")

        # è°ƒç”¨å‘Šè­¦å›è°ƒ
        for callback in self._alert_callbacks:
            try:
                callback(alert)
            except Exception as e:
                logger.error(f"å‘Šè­¦å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")

    def _get_latest_value(self, metric_type: MetricType) -> float:
        """è·å–æŒ‡æ ‡æœ€æ–°å€¼"""
        with self._metrics_lock:
            points = self._metrics_history[metric_type]
            return points[-1].value if points else 0.0

    def _auto_tune(self) -> None:
        """æ‰§è¡Œè‡ªåŠ¨è°ƒä¼˜"""
        for metric_type, rule in self._tuning_rules.items():
            latest_value = self._get_latest_value(metric_type)
            warning_threshold = rule['warning_threshold']

            # å¯¹äº"è¶Šä½è¶Šå¥½"çš„æŒ‡æ ‡
            if metric_type in [MetricType.LATENCY, MetricType.CPU_USAGE, MetricType.MEMORY_USAGE]:
                if latest_value >= warning_threshold:
                    auto_action = rule['auto_action']
                    if auto_action:
                        auto_action(latest_value, warning_threshold)

            # å¯¹äº"è¶Šé«˜è¶Šå¥½"çš„æŒ‡æ ‡
            else:
                if latest_value <= warning_threshold:
                    auto_action = rule['auto_action']
                    if auto_action:
                        auto_action(latest_value, warning_threshold)

    def _tune_cpu_usage(self, current_value: float, threshold: float) -> None:
        """CPUä½¿ç”¨ç‡è°ƒä¼˜"""
        logger.warning(f"ğŸ”§ CPUä½¿ç”¨ç‡è¿‡é«˜ ({current_value:.1f}%)ï¼Œæ‰§è¡Œè°ƒä¼˜")
        # è°ƒä¼˜ç­–ç•¥:
        # - é™ä½å¤„ç†é¢‘ç‡
        # - å‡å°‘å¹¶å‘æ•°
        # - å¯ç”¨èŠ‚èƒ½æ¨¡å¼

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        with self._metrics_lock:
            self._stats['auto_tuning_count'] += 1

    def _tune_memory_usage(self, current_value: float, threshold: float) -> None:
        """å†…å­˜ä½¿ç”¨é‡è°ƒä¼˜"""
        logger.warning(f"ğŸ”§ å†…å­˜ä½¿ç”¨é‡è¿‡é«˜ ({current_value:.1f}MB)ï¼Œæ‰§è¡Œè°ƒä¼˜")
        # è°ƒä¼˜ç­–ç•¥:
        # - è§¦å‘åƒåœ¾å›æ”¶
        # - é‡Šæ”¾ç¼“å­˜
        # - æ¸…ç†ä¸´æ—¶æ•°æ®

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        with self._metrics_lock:
            self._stats['auto_tuning_count'] += 1

    def _tune_latency(self, current_value: float, threshold: float) -> None:
        """å»¶è¿Ÿè°ƒä¼˜"""
        logger.warning(f"ğŸ”§ è¯†åˆ«å»¶è¿Ÿè¿‡é«˜ ({current_value:.1f}ms)ï¼Œæ‰§è¡Œè°ƒä¼˜")
        # è°ƒä¼˜ç­–ç•¥:
        # - å¯ç”¨NPUåŠ é€Ÿ
        # - ä¼˜åŒ–æ¨¡å‹å‚æ•°
        # - å‡å°‘éŸ³é¢‘ç¼“å†²å¤§å°

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        with self._metrics_lock:
            self._stats['auto_tuning_count'] += 1

    def _tune_accuracy(self, current_value: float, threshold: float) -> None:
        """å‡†ç¡®ç‡è°ƒä¼˜"""
        logger.warning(f"ğŸ”§ è¯†åˆ«å‡†ç¡®ç‡è¿‡ä½ ({current_value:.1%})ï¼Œæ‰§è¡Œè°ƒä¼˜")
        # è°ƒä¼˜ç­–ç•¥:
        # - è°ƒæ•´æ¨¡å‹å‚æ•°
        # - å¢å¼ºéŸ³é¢‘é¢„å¤„ç†
        # - æ›´æ–°è¯­è¨€æ¨¡å‹

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        with self._metrics_lock:
            self._stats['auto_tuning_count'] += 1

    def _tune_wake_word(self, current_value: float, threshold: float) -> None:
        """å”¤é†’è¯æ£€æµ‹è°ƒä¼˜"""
        logger.warning(f"ğŸ”§ å”¤é†’è¯æ£€æµ‹æˆåŠŸç‡è¿‡ä½ ({current_value:.1%})ï¼Œæ‰§è¡Œè°ƒä¼˜")
        # è°ƒä¼˜ç­–ç•¥:
        # - è°ƒæ•´æ£€æµ‹é˜ˆå€¼
        # - ä¼˜åŒ–ç‰¹å¾æå–
        # - æ›´æ–°å”¤é†’è¯æ¨¡å‹

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        with self._metrics_lock:
            self._stats['auto_tuning_count'] += 1

    def _establish_baseline(self, duration: float = 30.0) -> None:
        """å»ºç«‹æ€§èƒ½åŸºçº¿"""
        logger.info("ğŸ“Š å»ºç«‹æ€§èƒ½åŸºçº¿...")

        # ç­‰å¾…é‡‡æ ·å®Œæˆ
        time.sleep(duration)

        # è®¡ç®—åŸºçº¿å€¼
        for metric_type in MetricType:
            if metric_type != MetricType.POWER_CONSUMPTION and metric_type != MetricType.TEMPERATURE:
                stats = self.get_metric_stats(metric_type)
                self._baseline[metric_type] = stats['mean']

        logger.info(f"ğŸ“Š æ€§èƒ½åŸºçº¿å»ºç«‹å®Œæˆ: {self._baseline}")

    def add_alert_callback(self, callback: Callable[[Alert], None]) -> None:
        """æ·»åŠ å‘Šè­¦å›è°ƒå‡½æ•°"""
        self._alert_callbacks.append(callback)

    def remove_alert_callback(self, callback: Callable[[Alert], None]) -> bool:
        """ç§»é™¤å‘Šè­¦å›è°ƒå‡½æ•°"""
        try:
            self._alert_callbacks.remove(callback)
            return True
        except ValueError:
            return False

    def get_recent_alerts(self, count: int = 10) -> List[Alert]:
        """è·å–æœ€è¿‘çš„å‘Šè­¦"""
        return self._alerts[-count:] if count > 0 else self._alerts

    def resolve_alert(self, alert_index: int) -> bool:
        """è§£å†³å‘Šè­¦"""
        if 0 <= alert_index < len(self._alerts):
            self._alerts[alert_index].resolved = True
            self._alerts[alert_index].resolved_at = time.time()
            with self._metrics_lock:
                self._stats['resolved_alerts'] += 1
            return True
        return False

    def export_metrics(self, output_path: str, format: str = 'json') -> bool:
        """
        å¯¼å‡ºæŒ‡æ ‡æ•°æ®

        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            format: æ ¼å¼ ('json' æˆ– 'csv')

        Returns:
            æ˜¯å¦æˆåŠŸå¯¼å‡º
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)

            if format == 'json':
                # å¯¼å‡ºä¸ºJSONæ ¼å¼
                data = {}
                for metric_type, points in self._metrics_history.items():
                    data[metric_type.name] = [
                        {
                            'timestamp': p.timestamp,
                            'value': p.value,
                            'metadata': p.metadata
                        }
                        for p in points
                    ]

                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)

            elif format == 'csv':
                # å¯¼å‡ºä¸ºCSVæ ¼å¼ (éœ€è¦è½¬æ¢ä¸ºäºŒç»´è¡¨æ ¼)
                pass  # ç®€åŒ–å®ç°

            logger.info(f"ğŸ“Š æŒ‡æ ‡æ•°æ®å·²å¯¼å‡º: {output_path}")
            return True

        except Exception as e:
            logger.error(f"å¯¼å‡ºæŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
            return False

    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        with self._metrics_lock:
            uptime = time.time() - self._stats['start_time']
            uptime_hours = uptime / 3600

            summary = {
                'uptime_seconds': uptime,
                'uptime_hours': uptime_hours,
                'total_samples': self._stats['total_samples'],
                'auto_tuning_count': self._stats['auto_tuning_count'],
                'alert_count': self._stats['alert_count'],
                'resolved_alerts': self._stats['resolved_alerts'],
                'baseline': self._baseline.copy()
            }

            # æ·»åŠ æœ€è¿‘1å°æ—¶çš„æŒ‡æ ‡ç»Ÿè®¡
            for metric_type in MetricType:
                if metric_type in self._baseline:
                    stats = self.get_metric_stats(metric_type, duration_seconds=3600)
                    summary[f'{metric_type.name}_1h'] = stats

            return summary

    def print_performance_report(self) -> None:
        """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        summary = self.get_performance_summary()
        uptime_hours = summary['uptime_hours']

        print("\n" + "=" * 60)
        print("æ€§èƒ½ç›‘æ§æŠ¥å‘Š")
        print("=" * 60)
        print(f"è¿è¡Œæ—¶é—´:     {uptime_hours:.2f} å°æ—¶")
        print(f"æ€»é‡‡æ ·æ•°:     {summary['total_samples']:,}")
        print(f"è‡ªåŠ¨è°ƒä¼˜:     {summary['auto_tuning_count']} æ¬¡")
        print(f"å‘Šè­¦æ€»æ•°:     {summary['alert_count']}")
        print(f"å·²è§£å†³å‘Šè­¦:   {summary['resolved_alerts']}")
        print("\nåŸºçº¿æŒ‡æ ‡:")
        for metric_type, baseline_value in summary['baseline'].items():
            print(f"  {metric_type:20s}: {baseline_value:.2f}")
        print("\næœ€è¿‘1å°æ—¶æ€§èƒ½:")
        for key, stats in summary.items():
            if key.endswith('_1h') and isinstance(stats, dict):
                metric_name = key.replace('_1h', '')
                print(f"  {metric_name:20s}: å‡å€¼={stats['mean']:.2f}, "
                     f"P95={stats['p95']:.2f}, P99={stats['p99']:.2f}")
        print("=" * 60)

    def __str__(self) -> str:
        summary = self.get_performance_summary()
        uptime_hours = summary['uptime_hours']

        return (
            f"æ€§èƒ½ç›‘æ§å™¨\n"
            f"è¿è¡Œæ—¶é—´: {uptime_hours:.2f}å°æ—¶\n"
            f"é‡‡æ ·æ•°: {summary['total_samples']:,}\n"
            f"è‡ªåŠ¨è°ƒä¼˜: {summary['auto_tuning_count']}æ¬¡\n"
            f"å‘Šè­¦: {summary['alert_count']} (å·²è§£å†³{summary['resolved_alerts']})\n"
            f"ç›‘æ§æŒ‡æ ‡: {len(self._tuning_rules)}é¡¹"
        )

    def __repr__(self) -> str:
        return (f"PerformanceMonitor(interval={self.sample_interval}s, "
                f"history={self.history_size}, "
                f"auto_tuning={self.enable_auto_tuning})")


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
    monitor = PerformanceMonitor(sample_interval=0.5, enable_auto_tuning=True)

    print("=" * 60)
    print("æ€§èƒ½ç›‘æ§å™¨æ¼”ç¤º")
    print("=" * 60)

    # æ·»åŠ å‘Šè­¦å›è°ƒ
    def on_alert(alert: Alert):
        print(f"ğŸš¨ å‘Šè­¦: {alert.level.name} - {alert.message}")

    monitor.add_alert_callback(on_alert)

    # æ¨¡æ‹Ÿè¿è¡Œ30ç§’
    print("\næ¨¡æ‹Ÿç³»ç»Ÿè¿è¡Œ30ç§’...")
    start_time = time.time()

    while time.time() - start_time < 30:
        # æ¨¡æ‹Ÿåº”ç”¨æŒ‡æ ‡
        import random

        # æ¨¡æ‹Ÿè¯†åˆ«å»¶è¿Ÿ
        latency = random.normalvariate(150, 30)  # å¹³å‡150msï¼Œæ ‡å‡†å·®30ms
        monitor._record_metric(MetricType.LATENCY, latency, metadata={'session': 'demo'})

        # æ¨¡æ‹Ÿè¯†åˆ«å‡†ç¡®ç‡
        accuracy = random.betavariate(8, 2)  # Betaåˆ†å¸ƒï¼Œåå‘é«˜å‡†ç¡®ç‡
        monitor._record_metric(MetricType.ACCURACY, accuracy)

        # æ¨¡æ‹Ÿå”¤é†’è¯æ£€æµ‹æˆåŠŸç‡
        wake_word_success = random.betavariate(15, 1)  # åå‘é«˜æˆåŠŸç‡
        monitor._record_metric(MetricType.WAKE_WORD_SUCCESS, wake_word_success)

        time.sleep(0.5)

    # æ‰“å°æ€§èƒ½æŠ¥å‘Š
    monitor.print_performance_report()

    # æ‰“å°ç‰¹å®šæŒ‡æ ‡ç»Ÿè®¡
    print("\nå…³é”®æŒ‡æ ‡ç»Ÿè®¡:")
    for metric_type in [MetricType.CPU_USAGE, MetricType.LATENCY, MetricType.ACCURACY]:
        stats = monitor.get_metric_stats(metric_type)
        print(f"\n{metric_type.name}:")
        print(f"  æ ·æœ¬æ•°: {stats['count']}")
        print(f"  å‡å€¼:   {stats['mean']:.2f}")
        print(f"  P95:    {stats['p95']:.2f}")
        print(f"  P99:    {stats['p99']:.2f}")

    # æ‰“å°æœ€è¿‘çš„å‘Šè­¦
    alerts = monitor.get_recent_alerts(5)
    if alerts:
        print("\næœ€è¿‘çš„å‘Šè­¦:")
        for i, alert in enumerate(alerts):
            resolved = "âœ…" if alert.resolved else "âŒ"
            print(f"  {i+1}. {resolved} [{alert.level.name}] {alert.message}")

    # å¯¼å‡ºæ•°æ®
    monitor.export_metrics('/tmp/performance_metrics.json')

    # æ¸…ç†
    monitor.stop_monitoring()
    print("\nâœ… æ€§èƒ½ç›‘æ§å™¨æ¼”ç¤ºç»“æŸ")
