#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Story 2.1: é€šä¹‰åƒé—®APIé›†æˆ - å“åº”è§£æå™¨

APIå“åº”è§£æå’Œå¤„ç†æ¨¡å—ï¼Œå®ç°å“åº”æ ¼å¼è§£æã€ç»“æœæå–ã€é”™è¯¯å¤„ç†å’Œæ ¼å¼åŒ–è¾“å‡ºã€‚
æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ã€æ–‡æœ¬æ¸…æ´—ã€è¯­è¨€æ£€æµ‹å’Œå†…å®¹åˆ†æã€‚

ä½œè€…: Dev Agent
æ•…äº‹ID: Story 2.1
Epic: 2 - æ™ºèƒ½å¯¹è¯æ¨¡å—
"""

import re
import json
import logging
from typing import Dict, Any, List, Optional, Union, Tuple
from dataclasses import dataclass
from enum import Enum
import jieba
import jieba.posseg as pseg
from langdetect import detect, LangDetectException

from .qwen_client import QwenResponse


logger = logging.getLogger(__name__)


class OutputFormat(Enum):
    """è¾“å‡ºæ ¼å¼"""
    PLAIN = "plain"              # çº¯æ–‡æœ¬
    MARKDOWN = "markdown"        # Markdownæ ¼å¼
    JSON = "json"                # JSONæ ¼å¼
    CONVERSATION = "conversation"  # å¯¹è¯æ ¼å¼
    STRUCTURED = "structured"    # ç»“æ„åŒ–æ•°æ®


class Language(Enum):
    """è¯­è¨€ç±»å‹"""
    CHINESE = "zh"
    ENGLISH = "en"
    CANTONESE = "yue"  # ç²¤è¯­
    MIXED = "mixed"
    UNKNOWN = "unknown"


@dataclass
class ParsedResponse:
    """è§£æåçš„å“åº”å¯¹è±¡"""
    text: str
    language: Language
    confidence: float
    word_count: int
    sentence_count: int
    has_code: bool
    has_links: bool
    sentiment: str
    topics: List[str]
    entities: List[Dict[str, str]]
    metadata: Dict[str, Any]
    raw_response: QwenResponse
    formatted_output: str
    output_format: OutputFormat


class ResponseParser:
    """
    APIå“åº”è§£æå™¨

    åŠŸèƒ½ç‰¹æ€§:
    - å“åº”å†…å®¹è§£æ
    - è¯­è¨€æ£€æµ‹
    - æ–‡æœ¬åˆ†æå’Œæ¸…æ´—
    - å®ä½“è¯†åˆ«
    - ä¸»é¢˜æå–
    - æƒ…æ„Ÿåˆ†æ
    - å¤šæ ¼å¼è¾“å‡º
    - é”™è¯¯å¤„ç†
    """

    def __init__(self):
        """åˆå§‹åŒ–å“åº”è§£æå™¨"""
        # åˆå§‹åŒ–åˆ†è¯å·¥å…·
        jieba.initialize()

        # å¸¸ç”¨åœç”¨è¯
        self.stop_words = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'é‡Œ', 'å', 'ä»¥', 'æ‰€', 'å¦‚æœ', 'å› ä¸º', 'ä½†æ˜¯'
        }

        # è¯­è¨€å…³é”®è¯
        self.chinese_keywords = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'é‡Œ', 'å', 'ä»¥', 'æ‰€', 'å¦‚æœ', 'å› ä¸º', 'ä½†æ˜¯', 'ä¹ˆ', 'å•¦', 'å‘€', 'å˜›', 'å‘¢'
        }

        self.cantonese_keywords = {
            'å˜…', 'å’—', 'å•¦', 'å–‡', 'å˜›', 'å‘¢', 'å’¯', 'å•«', 'å’‹', 'å’ª', 'å””', 'å†‡', 'ä½¢', 'å˜…', 'å˜…', 'ä¹‹', 'å’', 'å’æ ·', 'å’å˜…', 'è¾¹åº¦', 'è¾¹ä¸ª', 'å’©', 'ä¹œ', 'è¾¹', 'å‘¢', 'å—°', 'å—»', 'å–º', 'åšŸ', 'å»', 'ç•ª', 'åšŸ', 'è¿”', 'å»', 'åšŸ', 'ç•ª', 'åšŸ', 'è¿”', 'å»', 'åšŸ', 'ç•ª', 'åšŸ', 'è¿”', 'å»'
        }

        logger.info("âœ… å“åº”è§£æå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info("   - è¯­è¨€æ£€æµ‹: æ”¯æŒä¸­æ–‡ã€ç²¤è¯­ã€è‹±æ–‡")
        logger.info("   - æ–‡æœ¬åˆ†æ: åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€åœç”¨è¯è¿‡æ»¤")
        logger.info("   - è¾“å‡ºæ ¼å¼: æ”¯æŒå¤šç§æ ¼å¼åŒ–è¾“å‡º")

    def parse_response(
        self,
        response: QwenResponse,
        output_format: OutputFormat = OutputFormat.PLAIN,
        language: Optional[Language] = None,
        include_metadata: bool = True
    ) -> ParsedResponse:
        """
        è§£æAPIå“åº”

        Args:
            response: é€šä¹‰åƒé—®å“åº”å¯¹è±¡
            output_format: è¾“å‡ºæ ¼å¼
            language: æŒ‡å®šè¯­è¨€ï¼ˆå¯é€‰ï¼‰
            include_metadata: æ˜¯å¦åŒ…å«å…ƒæ•°æ®

        Returns:
            ParsedResponse: è§£æåçš„å“åº”å¯¹è±¡
        """
        try:
            # åŸºæœ¬è§£æ
            raw_text = response.text.strip()

            # è¯­è¨€æ£€æµ‹
            detected_language = self._detect_language(raw_text, language)

            # æ–‡æœ¬åˆ†æ
            word_count = self._count_words(raw_text, detected_language)
            sentence_count = self._count_sentences(raw_text, detected_language)

            # å†…å®¹ç‰¹å¾æ£€æµ‹
            has_code = self._detect_code(raw_text)
            has_links = self._detect_links(raw_text)

            # ä¸»é¢˜å’Œå®ä½“æå–
            topics = self._extract_topics(raw_text, detected_language)
            entities = self._extract_entities(raw_text, detected_language)

            # æƒ…æ„Ÿåˆ†æ
            sentiment = self._analyze_sentiment(raw_text, detected_language)

            # ç½®ä¿¡åº¦è®¡ç®—
            confidence = self._calculate_confidence(response)

            # æ ¼å¼åŒ–è¾“å‡º
            formatted_output = self._format_output(
                raw_text, output_format, detected_language
            )

            # æ„å»ºå…ƒæ•°æ®
            metadata = {}
            if include_metadata:
                metadata = {
                    'model': response.model,
                    'usage': response.usage,
                    'finish_reason': response.finish_reason,
                    'request_id': response.request_id,
                    'processing_time': self._estimate_processing_time(response),
                    'content_features': {
                        'word_count': word_count,
                        'sentence_count': sentence_count,
                        'has_code': has_code,
                        'has_links': has_links,
                        'language': detected_language.value
                    }
                }

            parsed_response = ParsedResponse(
                text=raw_text,
                language=detected_language,
                confidence=confidence,
                word_count=word_count,
                sentence_count=sentence_count,
                has_code=has_code,
                has_links=has_links,
                sentiment=sentiment,
                topics=topics,
                entities=entities,
                metadata=metadata,
                raw_response=response,
                formatted_output=formatted_output,
                output_format=output_format
            )

            logger.info(f"âœ… å“åº”è§£æå®Œæˆ")
            logger.info(f"   - è¯­è¨€: {detected_language.value}")
            logger.info(f"   - è¯æ•°: {word_count}")
            logger.info(f"   - å¥å­æ•°: {sentence_count}")
            logger.info(f"   - æ ¼å¼: {output_format.value}")

            return parsed_response

        except Exception as e:
            logger.error(f"âŒ å“åº”è§£æå¤±è´¥: {e}")
            raise ValueError(f"æ— æ³•è§£æå“åº”: {e}")

    def _detect_language(
        self,
        text: str,
        specified_language: Optional[Language] = None
    ) -> Language:
        """æ£€æµ‹è¯­è¨€ç±»å‹"""
        if specified_language:
            return specified_language

        # é¢„å¤„ç†æ–‡æœ¬
        text = text.lower().strip()

        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å…³é”®è¯
        chinese_count = sum(1 for word in self.chinese_keywords if word in text)
        cantonese_count = sum(1 for word in self.cantonese_keywords if word in text)

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è‹±æ–‡
        english_pattern = re.compile(r'[a-zA-Z]{3,}')
        english_matches = english_pattern.findall(text)

        # åˆ¤æ–­è¯­è¨€
        if cantonese_count > chinese_count and cantonese_count > 0:
            return Language.CANTONESE
        elif chinese_count > 0:
            return Language.CHINESE
        elif len(english_matches) > len(text.split()) * 0.5:
            return Language.ENGLISH
        elif chinese_count > 0 and len(english_matches) > 0:
            return Language.MIXED
        else:
            try:
                detected = detect(text)
                if detected == 'zh':
                    return Language.CHINESE
                elif detected == 'en':
                    return Language.ENGLISH
                else:
                    return Language.UNKNOWN
            except LangDetectException:
                return Language.UNKNOWN

    def _count_words(self, text: str, language: Language) -> int:
        """è®¡ç®—è¯æ•°"""
        if language == Language.CHINESE or language == Language.CANTONESE:
            # ä¸­æ–‡ä½¿ç”¨ç»“å·´åˆ†è¯
            words = jieba.cut(text)
            words = [w for w in words if w.strip() and w not in self.stop_words]
            return len(words)
        else:
            # è‹±æ–‡æŒ‰ç©ºæ ¼åˆ†è¯
            words = text.split()
            words = [w for w in words if w.strip() and w.lower() not in self.stop_words]
            return len(words)

    def _count_sentences(self, text: str, language: Language) -> int:
        """è®¡ç®—å¥å­æ•°"""
        if language == Language.CHINESE or language == Language.CANTONESE:
            # ä¸­æ–‡æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
            return len([s for s in sentences if s.strip()])
        else:
            # è‹±æ–‡æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
            sentences = re.split(r'[.!?]+', text)
            return len([s for s in sentences if s.strip()])

    def _detect_code(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åŒ…å«ä»£ç """
        code_patterns = [
            r'```[\s\S]*?```',  # ä»£ç å—
            r'`[^`]+`',        # è¡Œå†…ä»£ç 
            r'def\s+\w+\(',    # Pythonå‡½æ•°å®šä¹‰
            r'class\s+\w+',    # ç±»å®šä¹‰
            r'import\s+\w+',   # å¯¼å…¥è¯­å¥
            r'function\s*\([^)]*\)',  # å‡½æ•°è°ƒç”¨
            r'console\.log\(', # JavaScript
            r'System\.out\.println'  # Java
        ]

        for pattern in code_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return True

        return False

    def _detect_links(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åŒ…å«é“¾æ¥"""
        url_pattern = re.compile(
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        )
        return bool(url_pattern.search(text))

    def _extract_topics(self, text: str, language: Language) -> List[str]:
        """æå–ä¸»é¢˜è¯"""
        topics = []

        if language == Language.CHINESE or language == Language.CANTONESE:
            # ä¸­æ–‡ä½¿ç”¨ç»“å·´åˆ†è¯
            words = pseg.cut(text)
            for word, flag in words:
                if (flag.startswith('n') or flag.startswith('v')) and len(word) > 1:
                    if word not in self.stop_words and not re.match(r'^[0-9]+$', word):
                        topics.append(word)
        else:
            # è‹±æ–‡æŒ‰è¯æ€§æå–
            words = text.split()
            for word in words:
                # ç®€å•çš„åè¯æå–ï¼ˆé¦–å­—æ¯å¤§å†™çš„è¯æˆ–åŒ…å«ç‰¹å®šåç¼€çš„è¯ï¼‰
                if (word[0].isupper() or word.endswith(('tion', 'ness', 'ment', 'ity', 'ism'))
                    and word.lower() not in self.stop_words):
                    topics.append(word)

        # å»é‡å¹¶è¿”å›å‰10ä¸ª
        return list(dict.fromkeys(topics))[:10]

    def _extract_entities(self, text: str, language: Language) -> List[Dict[str, str]]:
        """æå–å‘½åå®ä½“"""
        entities = []

        if language == Language.CHINESE or language == Language.CANTONESE:
            # ä¸­æ–‡å®ä½“è¯†åˆ«
            words = pseg.cut(text)
            for word, flag in words:
                if flag.startswith('nr'):  # äººå
                    entities.append({'type': 'person', 'text': word})
                elif flag.startswith('ns'):  # åœ°å
                    entities.append({'type': 'location', 'text': word})
                elif flag.startswith('nt'):  # æœºæ„å
                    entities.append({'type': 'organization', 'text': word})
        else:
            # ç®€å•çš„è‹±æ–‡å®ä½“è¯†åˆ«
            # åŒ¹é…å¤§å†™å¼€å¤´çš„è¯ï¼ˆäººåã€åœ°åã€æœºæ„åï¼‰
            entity_patterns = [
                (r'\b[A-Z][a-z]+\b', 'person'),
                (r'\b(?:Inc|LLC|Corp|Ltd|Corporation|Company)\b', 'organization'),
                (r'\b(?:New York|Los Angeles|Chicago|London|Paris|Tokyo)\b', 'location')
            ]

            for pattern, entity_type in entity_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    entities.append({'type': entity_type, 'text': match})

        return entities

    def _analyze_sentiment(self, text: str, language: Language) -> str:
        """æƒ…æ„Ÿåˆ†æ"""
        positive_words = {
            'zh': ['å¥½', 'æ£’', 'ä¼˜ç§€', 'å–œæ¬¢', 'é«˜å…´', 'æ»¡æ„', 'èµ', 'ä¸é”™', 'å‰å®³', 'å®Œç¾'],
            'en': ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'perfect', 'love', 'like', 'happy']
        }

        negative_words = {
            'zh': ['å', 'å·®', 'ç³Ÿç³•', 'è®¨åŒ', 'ç”Ÿæ°”', 'ä¸æ»¡æ„', 'çƒ‚', 'å·®åŠ²', 'ä¸è¡Œ', 'ç³Ÿç³•'],
            'en': ['bad', 'terrible', 'awful', 'hate', 'angry', 'sad', 'disgusting', 'horrible', 'worst', 'dislike']
        }

        # æ ¹æ®è¯­è¨€é€‰æ‹©è¯å…¸
        if language == Language.CHINESE or language == Language.CANTONESE:
            pos_words = positive_words['zh']
            neg_words = negative_words['zh']
        else:
            pos_words = positive_words['en']
            neg_words = negative_words['en']

        text_lower = text.lower()

        pos_count = sum(1 for word in pos_words if word in text_lower)
        neg_count = sum(1 for word in neg_words if word in text_lower)

        if pos_count > neg_count:
            return 'positive'
        elif neg_count > pos_count:
            return 'negative'
        else:
            return 'neutral'

    def _calculate_confidence(self, response: QwenResponse) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        # åŸºäºå“åº”é•¿åº¦ã€ä½¿ç”¨æƒ…å†µç­‰è®¡ç®—ç½®ä¿¡åº¦
        text_length = len(response.text)

        # é•¿åº¦å¾—åˆ† (0-0.3)
        length_score = min(0.3, text_length / 1000)

        # ä½¿ç”¨æƒ…å†µå¾—åˆ† (0-0.4)
        usage = response.usage
        if usage:
            input_tokens = usage.get('input_tokens', 0)
            output_tokens = usage.get('output_tokens', 0)
            total_tokens = input_tokens + output_tokens
            usage_score = min(0.4, total_tokens / 10000)
        else:
            usage_score = 0.2

        # ç»“æŸåŸå› å¾—åˆ† (0-0.3)
        if response.finish_reason == 'stop':
            reason_score = 0.3
        elif response.finish_reason == 'length':
            reason_score = 0.2
        else:
            reason_score = 0.1

        total_confidence = length_score + usage_score + reason_score

        # é™åˆ¶åœ¨0-1èŒƒå›´å†…
        return min(1.0, max(0.0, total_confidence))

    def _estimate_processing_time(self, response: QwenResponse) -> float:
        """ä¼°ç®—å¤„ç†æ—¶é—´"""
        # åŸºäºtokensæ•°é‡ä¼°ç®—å¤„ç†æ—¶é—´
        usage = response.usage
        if usage:
            total_tokens = usage.get('input_tokens', 0) + usage.get('output_tokens', 0)
            # å‡è®¾æ¯1000 tokenséœ€è¦1ç§’å¤„ç†æ—¶é—´
            estimated_time = total_tokens / 1000
            return round(estimated_time, 2)
        return 0.0

    def _format_output(
        self,
        text: str,
        output_format: OutputFormat,
        language: Language
    ) -> str:
        """æ ¼å¼åŒ–è¾“å‡º"""
        if output_format == OutputFormat.PLAIN:
            return text.strip()

        elif output_format == OutputFormat.MARKDOWN:
            # åŸºæœ¬Markdownæ ¼å¼åŒ–
            formatted = text

            # æ£€æµ‹ä»£ç å—
            if '```' not in formatted:
                formatted = formatted.replace('```', '\n```')

            # æ£€æµ‹æ ‡é¢˜
            if language == Language.CHINESE or language == Language.CANTONESE:
                formatted = re.sub(r'^(.+?)[ã€‚ï¼ï¼Ÿ]', r'## \1', formatted, flags=re.MULTILINE)
            else:
                formatted = re.sub(r'^(.+?)[.!?]', r'## \1', formatted, flags=re.MULTILINE)

            return formatted

        elif output_format == OutputFormat.JSON:
            # JSONæ ¼å¼åŒ–
            return json.dumps({
                'text': text,
                'language': language.value,
                'formatted': True
            }, ensure_ascii=False, indent=2)

        elif output_format == OutputFormat.CONVERSATION:
            # å¯¹è¯æ ¼å¼
            return f"ğŸ¤– åŠ©æ‰‹: {text}"

        elif output_format == OutputFormat.STRUCTURED:
            # ç»“æ„åŒ–è¾“å‡º
            lines = text.split('\n')
            structured = []

            for line in lines:
                line = line.strip()
                if line:
                    structured.append(f"- {line}")

            return '\n'.join(structured)

        else:
            return text

    def batch_parse(
        self,
        responses: List[QwenResponse],
        output_format: OutputFormat = OutputFormat.PLAIN
    ) -> List[ParsedResponse]:
        """
        æ‰¹é‡è§£æå“åº”

        Args:
            responses: å“åº”å¯¹è±¡åˆ—è¡¨
            output_format: è¾“å‡ºæ ¼å¼

        Returns:
            List[ParsedResponse]: è§£æåçš„å“åº”åˆ—è¡¨
        """
        results = []

        for response in responses:
            try:
                parsed = self.parse_response(response, output_format)
                results.append(parsed)
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡è§£æä¸­å‡ºé”™: {e}")
                # åˆ›å»ºé”™è¯¯å“åº”
                error_parsed = ParsedResponse(
                    text="",
                    language=Language.UNKNOWN,
                    confidence=0.0,
                    word_count=0,
                    sentence_count=0,
                    has_code=False,
                    has_links=False,
                    sentiment="neutral",
                    topics=[],
                    entities=[],
                    metadata={'error': str(e)},
                    raw_response=response,
                    formatted_output="è§£æå¤±è´¥",
                    output_format=output_format
                )
                results.append(error_parsed)

        logger.info(f"âœ… æ‰¹é‡è§£æå®Œæˆ: {len(results)}ä¸ªå“åº”")
        return results

    def validate_response(self, response: QwenResponse) -> Tuple[bool, str]:
        """
        éªŒè¯å“åº”æœ‰æ•ˆæ€§

        Args:
            response: å“åº”å¯¹è±¡

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        if not response.text:
            return False, "å“åº”æ–‡æœ¬ä¸ºç©º"

        if len(response.text.strip()) < 1:
            return False, "å“åº”æ–‡æœ¬é•¿åº¦ä¸è¶³"

        if response.confidence < 0.0 or response.confidence > 1.0:
            return False, "ç½®ä¿¡åº¦å€¼æ— æ•ˆ"

        return True, "å“åº”æœ‰æ•ˆ"


# ROS2èŠ‚ç‚¹é›†æˆ
class ResponseParserNode:
    """å“åº”è§£æå™¨ROS2èŠ‚ç‚¹"""

    def __init__(self, node):
        """
        åˆå§‹åŒ–è§£æå™¨èŠ‚ç‚¹

        Args:
            node: ROS2èŠ‚ç‚¹å®ä¾‹
        """
        self.parser = ResponseParser()
        self.node = node

    def parse_and_publish(self, response: QwenResponse) -> ParsedResponse:
        """
        è§£æå“åº”å¹¶å‘å¸ƒç»“æœ

        Args:
            response: é€šä¹‰åƒé—®å“åº”å¯¹è±¡

        Returns:
            ParsedResponse: è§£æåçš„å“åº”å¯¹è±¡
        """
        try:
            parsed = self.parser.parse_response(response)

            self.node.get_logger().info(f"ğŸ“ è§£æå“åº”å®Œæˆ")
            self.node.get_logger().info(f"   - è¯­è¨€: {parsed.language.value}")
            self.node.get_logger().info(f"   - è¯æ•°: {parsed.word_count}")
            self.node.get_logger().info(f"   - ä¸»é¢˜: {len(parsed.topics)}ä¸ª")

            return parsed

        except Exception as e:
            self.node.get_logger().error(f"âŒ å“åº”è§£æå¤±è´¥: {e}")
            raise


if __name__ == '__main__':
    # ç¤ºä¾‹ç”¨æ³•
    from .qwen_client import QwenResponse

    # åˆ›å»ºæµ‹è¯•å“åº”
    test_response = QwenResponse(
        text="ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ã€‚æˆ‘å¯ä»¥å¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€æä¾›å»ºè®®ï¼Œæˆ–è€…è¿›è¡Œå¯¹è¯ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ",
        model="qwen3-vl-plus",
        usage={"input_tokens": 10, "output_tokens": 50},
        finish_reason="stop",
        request_id="test-123"
    )

    # åˆå§‹åŒ–è§£æå™¨
    parser = ResponseParser()

    # è§£æå“åº”
    parsed = parser.parse_response(test_response, OutputFormat.CONVERSATION)

    print(f"ğŸ“ åŸå§‹æ–‡æœ¬: {parsed.text}")
    print(f"ğŸŒ è¯­è¨€: {parsed.language.value}")
    print(f"ğŸ“Š ç½®ä¿¡åº¦: {parsed.confidence:.2f}")
    print(f"ğŸ“ è¯æ•°: {parsed.word_count}")
    print(f"ğŸ“‘ å¥å­æ•°: {parsed.sentence_count}")
    print(f"ğŸ’­ æƒ…æ„Ÿ: {parsed.sentiment}")
    print(f"ğŸ·ï¸ ä¸»é¢˜: {parsed.topics}")
    print(f"ğŸ”– å®ä½“: {parsed.entities}")
    print(f"ğŸ“„ æ ¼å¼åŒ–è¾“å‡º:\n{parsed.formatted_output}")
    print(f"ğŸ“Š å…ƒæ•°æ®: {json.dumps(parsed.metadata, indent=2, ensure_ascii=False)}")
