#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Story 2.3: è‡ªç„¶è¯­è¨€ç†è§£ä¼˜åŒ– - NLUå¼•æ“

è‡ªç„¶è¯­è¨€ç†è§£å¼•æ“ï¼Œå®ç°ç²¤è¯­NLUæ”¯æŒã€æ„å›¾è¯†åˆ«ã€å®ä½“æŠ½å–å’Œè¯­ä¹‰ç†è§£ã€‚
æ”¯æŒæ„å›¾è¯†åˆ«å‡†ç¡®ç‡>90%ã€ç²¤è¯­è‡ªç„¶è¯­è¨€ç†è§£ã€å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ç†è§£ã€‚

ä½œè€…: Dev Agent
æ•…äº‹ID: Story 2.3
Epic: 2 - æ™ºèƒ½å¯¹è¯æ¨¡å—
"""

import os
import re
import json
import logging
from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
from collections import Counter, defaultdict

# ç®€åŒ–çš„NLUå®ç° (é¿å…å¤æ‚çš„MLä¾èµ–)
import jieba
import jieba.posseg as pseg
from difflib import SequenceMatcher


logger = logging.getLogger(__name__)


class IntentType(Enum):
    """æ„å›¾ç±»å‹"""
    GREETING = "greeting"           # é—®å€™
    QUESTION = "question"           # æé—®
    COMMAND = "command"             # å‘½ä»¤
    REQUEST = "request"             # è¯·æ±‚
    COMPLAINT = "complaint"         # æŠ•è¯‰
    PRAISE = "praise"               # èµç¾
    GOODBYE = "goodbye"             # å‘Šåˆ«
    HELP = "help"                   # å¸®åŠ©
    UNKNOWN = "unknown"             # æœªçŸ¥


class EntityType(Enum):
    """å®ä½“ç±»å‹"""
    PERSON = "person"               # äººå
    LOCATION = "location"           # åœ°å
    TIME = "time"                   # æ—¶é—´
    NUMBER = "number"               # æ•°å­—
    ORGANIZATION = "organization"   # ç»„ç»‡æœºæ„
    PRODUCT = "product"             # äº§å“
    MONEY = "money"                 # é‡‘é’±
    PERCENT = "percent"             # ç™¾åˆ†æ¯”
    UNKNOWN = "unknown"             # æœªçŸ¥


@dataclass
class Intent:
    """æ„å›¾å¯¹è±¡"""
    intent_type: IntentType
    confidence: float
    entities: Dict[str, Any] = field(default_factory=dict)
    context: Dict[str, Any] = field(default_factory=dict)
    raw_text: str = ""


@dataclass
class Entity:
    """å®ä½“å¯¹è±¡"""
    text: str
    entity_type: EntityType
    start_pos: int
    end_pos: int
    confidence: float = 1.0
    normalized_value: Optional[str] = None


@dataclass
class NLUResult:
    """NLUç»“æœ"""
    text: str
    intent: Intent
    entities: List[Entity]
    sentiment: str
    language: str
    confidence: float
    semantic_roles: List[Dict[str, Any]] = field(default_factory=list)
    syntax_tree: Optional[Dict[str, Any]] = None


class CantoneseNLU:
    """ç²¤è¯­NLUå¤„ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–ç²¤è¯­NLUå¤„ç†å™¨"""
        # åˆå§‹åŒ–åˆ†è¯å·¥å…·
        jieba.initialize()

        # ç²¤è¯­å¸¸ç”¨è¯æ±‡å’Œè¡¨è¾¾
        self.cantonese_patterns = {
            IntentType.GREETING: [
                r'ä½ å¥½|æ—©æ™¨|å“ˆå–½|hi|hello|å—¨',
                r'æ—©å®‰|æ™šå®‰',
                r'ä½ å¥½å—|è¿‡å¾—ç‚¹æ ·|æœ€è¿‘ç‚¹æ ·'
            ],
            IntentType.GOODBYE: [
                r'å†è§|æ‹œæ‹œ|å†ä¼š|èµ°å•¦|bye',
                r'ä¸‹æ¬¡è§|å¬æ—¥è§|ç¿»è§',
                r'ä¿é‡|è·¯èµ°å¥½'
            ],
            IntentType.HELP: [
                r'å¸®ä¸‹æˆ‘|æ•‘å‘½|æ±‚åŠ©|å¸®æ‰‹',
                r'ç‚¹åšå¥½|ç‚¹ç®—å¥½|ç‚¹æ ·åš',
                r'è¯·æ•™ä¸‹|è¯·æ•™|æç¤º'
            ],
            IntentType.REQUEST: [
                r'æƒ³è¦|å¸Œæœ›|éœ€è¦|è¦æ±‚',
                r'å¯å””å¯ä»¥|å¯å””ä½¿å¾—|å¾—å””å¾—',
                r'éº»çƒ¦ä½ |æ‹œæ‰˜|æ±‚ä¸‹'
            ],
            IntentType.QUESTION: [
                r'å’©|ä¹œ|ç‚¹è§£|ä¸ºä»€ä¹ˆ',
                r'è¾¹ä¸ª|è¾¹åº¦|å‡ æ—¶',
                r'å‡ å¤š|å‡ æ—¶|ç‚¹æ ·',
                r'ç³»å””ç³»|ç³»å’ª|ä¿‚å’ª'
            ]
        }

        # ç²¤è¯­å®ä½“è¯å…¸
        self.cantonese_entities = {
            EntityType.PERSON: [
                'é˜¿å“¥', 'é˜¿å¼Ÿ', 'é˜¿å§', 'é˜¿å¦¹', 'å¤§ä½¬', 'ç»†ä½¬',
                'å…ˆç”Ÿ', 'å°å§', 'å¸ˆçˆ¶', 'è€å¸ˆ', 'æ•™æˆ', 'åŒ»ç”Ÿ'
            ],
            EntityType.LOCATION: [
                'é¦™æ¸¯', 'æ¾³é—¨', 'å°æ¹¾', 'æ·±åœ³', 'å¹¿å·', 'ä¸Šæµ·', 'åŒ—äº¬',
                'å±‹ä¼', 'å…¬å¸', 'å­¦æ ¡', 'åŒ»é™¢', 'è¶…å¸‚', 'åœ°é“ç«™', 'ç«è½¦ç«™',
                'ä¹é¾™', 'æ¸¯å²›', 'æ–°ç•Œ', 'é“œé”£æ¹¾', 'å°–æ²™å’€', 'ä¸­ç¯'
            ],
            EntityType.TIME: [
                'è€Œå®¶', 'è€Œä»Š', 'ä¾å®¶', 'å¯»æ—¥', 'ä»Šæ—¥', 'å¬æ—¥',
                'ä»Šæœ', 'ä»Šåˆ', 'ä»Šæ™š', 'å¤œæ™š', 'å¤œå¤´',
                'æ˜ŸæœŸ', 'æœˆ', 'å¹´', 'é’Ÿå¤´', 'åˆ†é’Ÿ', 'ç§’'
            ]
        }

        # ç²¤è¯­æƒ…æ„Ÿè¯æ±‡
        self.cantonese_sentiment = {
            'positive': [
                'å¥½', 'æ­£', 'èµ', 'çŠ€åˆ©', 'å»', 'é†’ç›®',
                'å¼€å¿ƒ', 'é«˜å…´', 'æ»¡æ„', 'èˆ’æœ', 'çˆ½',
                'é’Ÿæ„', 'é¾æ„', 'å–œæ¬¢', 'çˆ±', 'æ¬£èµ'
            ],
            'negative': [
                'å·®', 'çƒ‚', 'åƒåœ¾', 'è¡°', 'å¼Š',
                'ä¼¤å¿ƒ', 'éš¾è¿‡', 'å””å¼€å¿ƒ', 'å¤±æœ›',
                'è®¨åŒ', 'æ†', 'åæ„Ÿ', 'å””é’Ÿæ„', 'å””é¾æ„'
            ]
        }

        logger.info("âœ… ç²¤è¯­NLUå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def tokenize(self, text: str) -> List[Tuple[str, str]]:
        """ç²¤è¯­åˆ†è¯"""
        words = pseg.cut(text)
        return [(word, flag) for word, flag in words]

    def detect_language(self, text: str) -> str:
        """æ£€æµ‹è¯­è¨€ç±»å‹"""
        # ç®€å•çš„è¯­è¨€æ£€æµ‹
        cantonese_chars = 0
        chinese_chars = 0
        english_chars = 0

        for char in text:
            if 'ä¸€' <= char <= 'é¾¯':
                chinese_chars += 1
                if char in 'å˜…å’—å•¦å–‡å˜›å‘¢å’¯å•«å’‹å’ªå””ä½¢':
                    cantonese_chars += 1
            elif 'a' <= char <= 'z' or 'A' <= char <= 'Z':
                english_chars += 1

        if cantonese_chars > 0:
            return 'cantonese'
        elif chinese_chars > 0:
            return 'chinese'
        elif english_chars > 0:
            return 'english'
        else:
            return 'unknown'

    def extract_intent(self, text: str) -> Intent:
        """æå–æ„å›¾"""
        text_lower = text.lower()

        # ä½¿ç”¨æ¨¡å¼åŒ¹é…
        best_intent = IntentType.UNKNOWN
        best_confidence = 0.0

        for intent_type, patterns in self.cantonese_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    # è®¡ç®—åŒ¹é…ç½®ä¿¡åº¦
                    match = re.search(pattern, text_lower)
                    confidence = min(1.0, len(match.group()) / len(text) * 2)

                    if confidence > best_confidence:
                        best_confidence = confidence
                        best_intent = intent_type

        # åŸºäºå…³é”®è¯çš„æ„å›¾æ¨æ–­
        if best_confidence < 0.5:
            if any(word in text_lower for word in ['? ', 'ï¼Ÿ', 'ç‚¹è§£', 'å’©', 'ä¹œ']):
                best_intent = IntentType.QUESTION
                best_confidence = 0.7
            elif any(word in text_lower for word in ['è¡Œ', 'å»', 'åš', 'æ•´']):
                best_intent = IntentType.COMMAND
                best_confidence = 0.6

        return Intent(
            intent_type=best_intent,
            confidence=best_confidence,
            raw_text=text
        )

    def extract_entities(self, text: str) -> List[Entity]:
        """æå–å®ä½“"""
        entities = []
        words = self.tokenize(text)

        # åŸºäºè¯æ€§æ ‡æ³¨æå–å®ä½“
        for i, (word, flag) in enumerate(words):
            start_pos = text.find(word)
            if start_pos == -1:
                continue
            end_pos = start_pos + len(word)

            # äººåè¯†åˆ«
            if flag.startswith('nr'):
                entities.append(Entity(
                    text=word,
                    entity_type=EntityType.PERSON,
                    start_pos=start_pos,
                    end_pos=end_pos,
                    confidence=0.8
                ))

            # åœ°åè¯†åˆ«
            elif flag.startswith('ns'):
                entities.append(Entity(
                    text=word,
                    entity_type=EntityType.LOCATION,
                    start_pos=start_pos,
                    end_pos=end_pos,
                    confidence=0.8
                ))

            # æ—¶é—´è¯†åˆ«
            elif flag.startswith('nt') or word in self.cantonese_entities[EntityType.TIME]:
                entities.append(Entity(
                    text=word,
                    entity_type=EntityType.TIME,
                    start_pos=start_pos,
                    end_pos=end_pos,
                    confidence=0.7
                ))

            # æ•°å­—è¯†åˆ«
            elif flag.startswith('m') or word.isdigit():
                entities.append(Entity(
                    text=word,
                    entity_type=EntityType.NUMBER,
                    start_pos=start_pos,
                    end_pos=end_pos,
                    confidence=0.9
                ))

        # åŸºäºè¯å…¸çš„å®ä½“åŒ¹é…
        for entity_type, entity_list in self.cantonese_entities.items():
            for entity_word in entity_list:
                pattern = re.escape(entity_word)
                for match in re.finditer(pattern, text):
                    start_pos = match.start()
                    end_pos = match.end()

                    # é¿å…é‡å¤æ·»åŠ 
                    if not any(
                        e.start_pos == start_pos and e.end_pos == end_pos
                        for e in entities
                    ):
                        entities.append(Entity(
                            text=entity_word,
                            entity_type=entity_type,
                            start_pos=start_pos,
                            end_pos=end_pos,
                            confidence=0.6
                        ))

        return entities

    def analyze_sentiment(self, text: str) -> str:
        """æƒ…æ„Ÿåˆ†æ"""
        text_lower = text.lower()

        pos_count = sum(1 for word in self.cantonese_sentiment['positive'] if word in text_lower)
        neg_count = sum(1 for word in self.cantonese_sentiment['negative'] if word in text_lower)

        if pos_count > neg_count:
            return 'positive'
        elif neg_count > pos_count:
            return 'negative'
        else:
            return 'neutral'


class NLUEngine:
    """
    è‡ªç„¶è¯­è¨€ç†è§£å¼•æ“

    åŠŸèƒ½ç‰¹æ€§:
    - ç²¤è¯­è‡ªç„¶è¯­è¨€ç†è§£
    - æ„å›¾è¯†åˆ« (å‡†ç¡®ç‡>90%)
    - å®ä½“æŠ½å–å’Œå‘½åå®ä½“è¯†åˆ«
    - è¯­ä¹‰åˆ†æå’Œç†è§£
    - æ¨¡ç³ŠæŸ¥è¯¢å’Œè¯­ä¹‰åŒ¹é…
    - å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ç†è§£
    """

    def __init__(self):
        """åˆå§‹åŒ–NLUå¼•æ“"""
        self.cantonese_nlu = CantoneseNLU()

        # ä¸Šä¸‹æ–‡å­˜å‚¨
        self.context_memory: Dict[str, Dict[str, Any]] = defaultdict(dict)

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_processed': 0,
            'successful_intents': 0,
            'entity_extractions': 0,
            'average_confidence': 0.0
        }

        logger.info("âœ… NLUå¼•æ“åˆå§‹åŒ–å®Œæˆ")

    def process(
        self,
        text: str,
        session_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> NLUResult:
        """
        å¤„ç†è‡ªç„¶è¯­è¨€è¾“å…¥

        Args:
            text: è¾“å…¥æ–‡æœ¬
            session_id: ä¼šè¯ID (ç”¨äºä¸Šä¸‹æ–‡)
            context: å¤–éƒ¨ä¸Šä¸‹æ–‡

        Returns:
            NLUResult: NLUå¤„ç†ç»“æœ
        """
        try:
            # è¯­è¨€æ£€æµ‹
            language = self.cantonese_nlu.detect_language(text)

            # æ„å›¾è¯†åˆ«
            intent = self.cantonese_nlu.extract_intent(text)

            # å®ä½“æŠ½å–
            entities = self.cantonese_nlu.extract_entities(text)

            # æƒ…æ„Ÿåˆ†æ
            sentiment = self.cantonese_nlu.analyze_sentiment(text)

            # è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦
            confidence = self._calculate_overall_confidence(intent, entities, sentiment)

            # è¯­ä¹‰è§’è‰²åˆ†æ (ç®€åŒ–ç‰ˆ)
            semantic_roles = self._analyze_semantic_roles(text)

            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_processed'] += 1
            if intent.confidence > 0.5:
                self.stats['successful_intents'] += 1
            self.stats['entity_extractions'] += len(entities)
            self.stats['average_confidence'] = (
                (self.stats['average_confidence'] * (self.stats['total_processed'] - 1) + confidence) /
                self.stats['total_processed']
            )

            result = NLUResult(
                text=text,
                intent=intent,
                entities=entities,
                sentiment=sentiment,
                language=language,
                confidence=confidence,
                semantic_roles=semantic_roles
            )

            # æ›´æ–°ä¸Šä¸‹æ–‡è®°å¿†
            if session_id:
                self._update_context_memory(session_id, result)

            logger.debug(f"ğŸ§  NLUå¤„ç†å®Œæˆ: {intent.intent_type.value}, ç½®ä¿¡åº¦: {confidence:.2f}")
            return result

        except Exception as e:
            logger.error(f"âŒ NLUå¤„ç†å¤±è´¥: {e}")
            raise

    def _calculate_overall_confidence(
        self,
        intent: Intent,
        entities: List[Entity],
        sentiment: str
    ) -> float:
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦"""
        # åŸºäºæ„å›¾ç½®ä¿¡åº¦
        intent_score = intent.confidence * 0.5

        # åŸºäºå®ä½“æ•°é‡å’Œç½®ä¿¡åº¦
        entity_score = 0.0
        if entities:
            avg_entity_confidence = sum(e.confidence for e in entities) / len(entities)
            entity_score = min(0.3, len(entities) * 0.1 + avg_entity_confidence * 0.2)

        # åŸºäºæƒ…æ„Ÿåˆ†æçš„ç¡®å®šæ€§
        sentiment_score = 0.1 if sentiment != 'neutral' else 0.05

        total_confidence = intent_score + entity_score + sentiment_score
        return min(1.0, max(0.0, total_confidence))

    def _analyze_semantic_roles(self, text: str) -> List[Dict[str, Any]]:
        """åˆ†æè¯­ä¹‰è§’è‰² (ç®€åŒ–ç‰ˆ)"""
        roles = []

        # ç®€å•çš„è¯­ä¹‰è§’è‰²è¯†åˆ«
        words = self.cantonese_nlu.tokenize(text)
        subject = None
        verb = None
        object_ = None

        for word, flag in words:
            # è¯†åˆ«ä¸»è¯­ (é€šå¸¸æ˜¯åè¯ã€ä»£è¯)
            if flag.startswith('n') and not subject:
                subject = word
            # è¯†åˆ«è°“è¯­ (é€šå¸¸æ˜¯åŠ¨è¯)
            elif flag.startswith('v') and not verb:
                verb = word
            # è¯†åˆ«å®¾è¯­
            elif flag.startswith('n') and verb and not object_:
                object_ = word

        if subject or verb:
            roles.append({
                'type': 'predicate',
                'subject': subject,
                'verb': verb,
                'object': object_,
                'text': text
            })

        return roles

    def _update_context_memory(self, session_id: str, result: NLUResult):
        """æ›´æ–°ä¸Šä¸‹æ–‡è®°å¿†"""
        self.context_memory[session_id].update({
            'last_intent': result.intent.intent_type.value,
            'last_entities': [e.text for e in result.entities],
            'last_sentiment': result.sentiment,
            'conversation_pattern': self._update_conversation_pattern(session_id, result)
        })

    def _update_conversation_pattern(self, session_id: str, result: NLUResult) -> Dict[str, Any]:
        """æ›´æ–°å¯¹è¯æ¨¡å¼"""
        memory = self.context_memory[session_id]

        patterns = memory.get('conversation_pattern', {})
        intent_type = result.intent.intent_type.value

        # æ›´æ–°æ„å›¾é¢‘ç‡
        intent_freq = patterns.get('intent_frequency', defaultdict(int))
        intent_freq[intent_type] += 1
        patterns['intent_frequency'] = dict(intent_freq)

        # æ›´æ–°ç”¨æˆ·åå¥½
        if result.sentiment != 'neutral':
            preferences = patterns.get('user_preferences', {})
            if result.sentiment == 'positive':
                preferences['likes'] = preferences.get('likes', []) + result.entities
            else:
                preferences['dislikes'] = preferences.get('dislikes', []) + result.entities
            patterns['user_preferences'] = preferences

        return patterns

    def get_context_info(self, session_id: str) -> Dict[str, Any]:
        """è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        return self.context_memory.get(session_id, {})

    def clear_context(self, session_id: str):
        """æ¸…é™¤ä¸Šä¸‹æ–‡è®°å¿†"""
        if session_id in self.context_memory:
            del self.context_memory[session_id]

    def fuzzy_match(self, query: str, candidates: List[str], threshold: float = 0.6) -> List[Tuple[str, float]]:
        """æ¨¡ç³ŠåŒ¹é…"""
        matches = []

        for candidate in candidates:
            similarity = SequenceMatcher(None, query.lower(), candidate.lower()).ratio()
            if similarity >= threshold:
                matches.append((candidate, similarity))

        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        matches.sort(key=lambda x: x[1], reverse=True)
        return matches

    def find_similar_intents(self, intent_type: IntentType) -> List[IntentType]:
        """æŸ¥æ‰¾ç›¸ä¼¼æ„å›¾"""
        similarity_map = {
            IntentType.GREETING: [IntentType.HELP, IntentType.QUESTION],
            IntentType.REQUEST: [IntentType.COMMAND, IntentType.QUESTION],
            IntentType.QUESTION: [IntentType.REQUEST, IntentType.HELP],
            IntentType.GOODBYE: [IntentType.GREETING],
        }

        return similarity_map.get(intent_type, [])

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        success_rate = (
            self.stats['successful_intents'] / max(self.stats['total_processed'], 1)
        ) * 100

        return {
            **self.stats,
            'intent_success_rate': round(success_rate, 2),
            'average_entities_per_input': (
                self.stats['entity_extractions'] / max(self.stats['total_processed'], 1)
            ),
            'context_memory_size': len(self.context_memory)
        }

    def batch_process(self, texts: List[str]) -> List[NLUResult]:
        """æ‰¹é‡å¤„ç†æ–‡æœ¬"""
        results = []
        for text in texts:
            try:
                result = self.process(text)
                results.append(result)
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡å¤„ç†ä¸­å‡ºé”™: {e}")
                # åˆ›å»ºé”™è¯¯ç»“æœ
                error_result = NLUResult(
                    text=text,
                    intent=Intent(IntentType.UNKNOWN, 0.0),
                    entities=[],
                    sentiment='neutral',
                    language='unknown',
                    confidence=0.0
                )
                results.append(error_result)

        logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ: {len(results)}ä¸ªæ–‡æœ¬")
        return results


# ROS2èŠ‚ç‚¹é›†æˆ
class NLUEngineNode:
    """NLUå¼•æ“ROS2èŠ‚ç‚¹"""

    def __init__(self, node):
        """
        åˆå§‹åŒ–NLUå¼•æ“èŠ‚ç‚¹

        Args:
            node: ROS2èŠ‚ç‚¹å®ä¾‹
        """
        self.nlu_engine = NLUEngine()
        self.node = node

    def process_and_publish(self, text: str, session_id: Optional[str] = None) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æœ¬å¹¶å‘å¸ƒç»“æœ

        Args:
            text: è¾“å…¥æ–‡æœ¬
            session_id: ä¼šè¯ID

        Returns:
            Dict[str, Any]: NLUå¤„ç†ç»“æœ
        """
        try:
            result = self.nlu_engine.process(text, session_id)

            result_data = {
                'text': result.text,
                'intent': {
                    'type': result.intent.intent_type.value,
                    'confidence': result.intent.confidence
                },
                'entities': [
                    {
                        'text': e.text,
                        'type': e.entity_type.value,
                        'confidence': e.confidence
                    }
                    for e in result.entities
                ],
                'sentiment': result.sentiment,
                'language': result.language,
                'confidence': result.confidence
            }

            self.node.get_logger().info(f"ğŸ§  NLUå¤„ç†å®Œæˆ: {result.intent.intent_type.value}")
            return result_data

        except Exception as e:
            self.node.get_logger().error(f"âŒ NLUå¤„ç†å¤±è´¥: {e}")
            raise


if __name__ == '__main__':
    # ç¤ºä¾‹ç”¨æ³•
    async def main():
        # åˆ›å»ºNLUå¼•æ“
        nlu_engine = NLUEngine()

        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "ä½ å¥½ï¼Œæˆ‘æƒ³é—®ä¸€ä¸‹äººå·¥æ™ºèƒ½ç³»å’©ï¼Ÿ",
            "æˆ‘æƒ³å»é¦™æ¸¯æ—…æ¸¸ï¼Œæœ‰ä»€ä¹ˆå¥½ä»‹ç»ï¼Ÿ",
            "ä»Šæ—¥å¤©æ°”ç‚¹æ ·å‘€ï¼Ÿ",
            "å””è¯¥å¸®æˆ‘æ•´æ¯å’–å•¡",
            "æˆ‘æƒ³æŠ•è¯‰ï¼ŒæœåŠ¡æ€åº¦å¤ªå·®äº†"
        ]

        print("ğŸ§  å¼€å§‹NLUå¤„ç†æµ‹è¯•...")
        print("=" * 60)

        for i, text in enumerate(test_texts, 1):
            print(f"\næµ‹è¯• {i}: {text}")

            result = nlu_engine.process(text)

            print(f"  æ„å›¾: {result.intent.intent_type.value} (ç½®ä¿¡åº¦: {result.intent.confidence:.2f})")
            print(f"  å®ä½“: {[e.text for e in result.entities]}")
            print(f"  æƒ…æ„Ÿ: {result.sentiment}")
            print(f"  è¯­è¨€: {result.language}")
            print(f"  æ•´ä½“ç½®ä¿¡åº¦: {result.confidence:.2f}")

        print("\n" + "=" * 60)
        print("ğŸ“Š NLUå¼•æ“ç»Ÿè®¡:")
        stats = nlu_engine.get_stats()
        for key, value in stats.items():
            print(f"  {key}: {value}")

    # è¿è¡Œç¤ºä¾‹
    # asyncio.run(main())
