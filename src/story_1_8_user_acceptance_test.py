#!/usr/bin/env python3.10
"""
XleRobot User Acceptance Test - ç²¤è¯­å®¶åº­ç”¨æˆ·éªŒæ”¶æµ‹è¯•
Story 1.8: ç³»ç»Ÿä¼˜åŒ–ä¸éƒ¨ç½²
BMad Method v6 Brownfield Level 4 ä¼ä¸šçº§æ ‡å‡†

åŠŸèƒ½ç‰¹æ€§:
- ç²¤è¯­å®¶åº­ç”¨æˆ·ä½“éªŒæµ‹è¯•
- å¤šæ¨¡æ€äº¤äº’éªŒæ”¶
- æ€§èƒ½ä½“éªŒè¯„ä¼°
- çœŸå®åœºæ™¯æ¨¡æ‹Ÿ
- ç”¨æˆ·åé¦ˆæ”¶é›†
- 100%ç¬¦åˆEpic 1çº¯åœ¨çº¿æ¶æ„
"""

import asyncio
import json
import time
import logging
import uuid
import statistics
from typing import Dict, Any, List, Optional, Tuple, Callable
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum
import numpy as np

logger = logging.getLogger(__name__)

class TestStatus(Enum):
    """æµ‹è¯•çŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"

class UserSatisfactionLevel(Enum):
    """ç”¨æˆ·æ»¡æ„åº¦ç­‰çº§"""
    EXCELLENT = "excellent"
    GOOD = "good"
    SATISFACTORY = "satisfactory"
    POOR = "poor"
    VERY_POOR = "very_poor"

@dataclass
class TestScenario:
    """æµ‹è¯•åœºæ™¯"""
    id: str
    name: str
    description: str
    category: str
    user_profile: str
    test_data: Dict[str, Any]
    expected_results: Dict[str, Any]
    max_response_time_ms: int = 3000
    weight: float = 1.0
    critical: bool = True

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    scenario_id: str
    status: TestStatus
    start_time: float
    end_time: Optional[float] = None
    response_time_ms: Optional[float] = None
    success: bool = False
    user_satisfaction: UserSatisfactionLevel = UserSatisfactionLevel.SATISFACTORY
    actual_results: Dict[str, Any] = field(default_factory=dict)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    user_feedback: Optional[str] = None
    error_message: Optional[str] = None

@dataclass
class UserAcceptanceTestSession:
    """ç”¨æˆ·éªŒæ”¶æµ‹è¯•ä¼šè¯"""
    session_id: str
    user_profile: Dict[str, Any]
    test_date: datetime
    scenarios: List[TestScenario]
    results: List[TestResult] = field(default_factory=list)
    overall_score: float = 0.0
    user_satisfaction: UserSatisfactionLevel = UserSatisfactionLevel.SATISFACTORY
    recommendations: List[str] = field(default_factory=list)
    status: TestStatus = TestStatus.PENDING

class CantoneseUserAcceptanceTest:
    """ç²¤è¯­å®¶åº­ç”¨æˆ·éªŒæ”¶æµ‹è¯• - Story 1.8æ ¸å¿ƒç»„ä»¶"""

    def __init__(self):
        """
        åˆå§‹åŒ–ç²¤è¯­å®¶åº­ç”¨æˆ·éªŒæ”¶æµ‹è¯•
        """
        logger.info("ğŸ§ª åˆå§‹åŒ–CantoneseUserAcceptanceTest - ç²¤è¯­å®¶åº­ç”¨æˆ·ä½“éªŒæµ‹è¯•")

        # æµ‹è¯•é…ç½®
        self.test_config = {
            "max_response_time_ms": 3000,
            "min_success_rate": 0.9,
            "min_satisfaction_score": 3.5,  # 5åˆ†åˆ¶
            "critical_scenarios_weight": 2.0,
            "performance_test_rounds": 5
        }

        # æµ‹è¯•ä¼šè¯
        self.current_session: Optional[UserAcceptanceTestSession] = None
        self.test_history: List[UserAcceptanceTestSession] = []

        # åˆ›å»ºæµ‹è¯•åœºæ™¯
        self.test_scenarios = self._create_test_scenarios()

        # æ€§èƒ½åŸºå‡†
        self.performance_baselines = {
            "asr_accuracy": 0.90,
            "tts_naturalness": 0.85,
            "vision_understanding": 0.80,
            "dialogue_coherence": 0.85,
            "response_time": 2000.0
        }

        logger.info("âœ… ç²¤è¯­å®¶åº­ç”¨æˆ·éªŒæ”¶æµ‹è¯•åˆå§‹åŒ–å®Œæˆ")

    def _create_test_scenarios(self) -> List[TestScenario]:
        """åˆ›å»ºæµ‹è¯•åœºæ™¯"""
        scenarios = [
            # åŸºç¡€è¯­éŸ³äº¤äº’åœºæ™¯
            TestScenario(
                id="basic_voice_greeting",
                name="åŸºç¡€è¯­éŸ³é—®å€™",
                description="ç”¨æˆ·è¯´æ—©æ™¨ï¼Œç³»ç»Ÿåº”è¯¥å›åº”é—®å€™",
                category="basic_interaction",
                user_profile="å®¶åº­ä¸»å¦‡",
                test_data={
                    "input_type": "voice",
                    "content": "æ—©æ™¨",
                    "context": {}
                },
                expected_results={
                    "response_contains": ["æ—©æ™¨", "ä½ å¥½"],
                    "response_cantonese": True,
                    "response_natural": True,
                    "max_response_time_ms": 2000
                },
                weight=1.5,
                critical=True
            ),

            # ç‰©å“è¯†åˆ«åœºæ™¯
            TestScenario(
                id="object_recognition_fruit",
                name="æ°´æœè¯†åˆ«",
                description="ç”¨æˆ·å±•ç¤ºè‹¹æœï¼Œç³»ç»Ÿåº”è¯†åˆ«å¹¶å›åº”",
                category="multimodal_vision",
                user_profile="å„¿ç«¥",
                test_data={
                    "input_type": "multimodal",
                    "audio": "æˆ‘å“‹é£Ÿå‘¢ä¸ª",
                    "image": "apple_image.jpg",
                    "context": {"scene": "kitchen"}
                },
                expected_results={
                    "identified_object": "è‹¹æœ",
                    "response_relevant": True,
                    "cantonese_terms": ["è‹¹æœ", "æ°´æœ"],
                    "child_friendly": True
                },
                weight=2.0,
                critical=True
            ),

            # å¤æ‚å¯¹è¯åœºæ™¯
            TestScenario(
                id="complex_dialogue_planning",
                name="å¯¹è¯å¼è®¡åˆ’",
                description="ç”¨æˆ·è®¨è®ºæ™šé¤è®¡åˆ’ï¼Œç³»ç»Ÿåº”ç†è§£å¹¶æä¾›å»ºè®®",
                category="complex_dialogue",
                user_profile="å®¶åº­ä¸»å¦‡",
                test_data={
                    "input_type": "voice",
                    "content": "ä»Šæ™šæƒ³ç…®é¤¸ï¼Œæœ‰å’©å¥½ä»‹ç»ï¼Ÿ",
                    "context": {"time": "evening", "meal": "dinner"}
                },
                expected_results={
                    "understands_context": True,
                    "provides_suggestions": True,
                    "cantonese_natural": True,
                    "relevant_suggestions": True
                },
                weight=1.8,
                critical=True
            ),

            # ç²¤è¯­æ–‡åŒ–åœºæ™¯
            TestScenario(
                id="cantonese_culture_festival",
                name="ç²¤è¯­æ–‡åŒ–èŠ‚æ—¥",
                description="ç”¨æˆ·è¯¢é—®èŠ‚æ—¥ä¿¡æ¯ï¼Œç³»ç»Ÿåº”ç†è§£ç²¤è¯­æ–‡åŒ–",
                category="cultural_understanding",
                user_profile="é•¿è€…",
                test_data={
                    "input_type": "voice",
                    "content": "ä¸­ç§‹èŠ‚å¿«åˆ°ï¼Œæœ‰å’©ä¼ ç»Ÿä¹ ä¿—ï¼Ÿ",
                    "context": {"festival": "mid_autumn", "culture": "cantonese"}
                },
                expected_results={
                    "cultural_understanding": True,
                    "cantonese_cultural_context": True,
                    "appropriate_response": True,
                    "respectful_tone": True
                },
                weight=1.5,
                critical=False
            ),

            # å¤šè½®å¯¹è¯åœºæ™¯
            TestScenario(
                id="multi_turn_shopping",
                name="å¤šè½®è´­ç‰©å¯¹è¯",
                description="ç”¨æˆ·è¯¢é—®è´­ç‰©ä¿¡æ¯ï¼Œç³»ç»Ÿåº”ä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡",
                category="multi_turn_dialogue",
                user_profile="å¹´è½»ç”¨æˆ·",
                test_data=[
                    {
                        "round": 1,
                        "content": "æˆ‘æƒ³ä¹°éƒ¨æ–°æ‰‹æœº",
                        "context": {}
                    },
                    {
                        "round": 2,
                        "content": "æœ‰å’©ç‰Œå­å¥½ï¼Ÿ",
                        "context": {"previous_topic": "phone_purchase"}
                    },
                    {
                        "round": 3,
                        "content": "Samsungå‘¢ä¸ªç‚¹æ ·ï¼Ÿ",
                        "context": {"brand_mentioned": "Samsung"}
                    }
                ],
                expected_results={
                    "maintains_context": True,
                    "remembers_previous_turns": True,
                    "progressive_understanding": True,
                    "natural_conversation": True
                },
                weight=2.0,
                critical=True
            ),

            # åº”æ€¥å“åº”åœºæ™¯
            TestScenario(
                id="emergency_response",
                name="åº”æ€¥å“åº”æµ‹è¯•",
                description="ç”¨æˆ·è¡¨è¾¾ç´§æ€¥æƒ…å†µï¼Œç³»ç»Ÿåº”å¿«é€Ÿå“åº”",
                category="safety_critical",
                user_profile="ä»»ä½•ç”¨æˆ·",
                test_data={
                    "input_type": "voice",
                    "content": "æœ‰ç«è­¦ï¼å¥½ç´§è¦ï¼",
                    "context": {"urgency": "high", "emergency": True}
                },
                expected_results={
                    "recognizes_emergency": True,
                    "quick_response": True,
                    "appropriate_emergency_response": True,
                    "response_time_ms": 1000
                },
                weight=3.0,
                critical=True
            ),

            # å„¿ç«¥å‹å¥½åœºæ™¯
            TestScenario(
                id="child_friendly_interaction",
                name="å„¿ç«¥å‹å¥½äº¤äº’",
                description="å„¿ç«¥ç”¨æˆ·ç”¨ç®€å•è¯­è¨€äº¤æµï¼Œç³»ç»Ÿåº”é€‚åº”",
                category="accessibility",
                user_profile="å„¿ç«¥(6-8å²)",
                test_data={
                    "input_type": "voice",
                    "content": "é˜¿å§ï¼Œé™ªæˆ‘ç©å•¦",
                    "context": {"user_age": 7, "friendly_tone": True}
                },
                expected_results={
                    "child_appropriate_language": True,
                    "friendly_response": True,
                    "patience_tone": True,
                    "engaging_response": True
                },
                weight=1.5,
                critical=False
            ),

            # æ€§èƒ½å‹åŠ›æµ‹è¯•
            TestScenario(
                id="performance_stress_test",
                name="æ€§èƒ½å‹åŠ›æµ‹è¯•",
                description="è¿ç»­å¿«é€Ÿæé—®ï¼Œæµ‹è¯•ç³»ç»Ÿå“åº”èƒ½åŠ›",
                category="performance",
                user_profile="å‹åŠ›æµ‹è¯•",
                test_data={
                    "input_type": "rapid_sequence",
                    "queries": [
                        "å®œå®¶å‡ å¤šç‚¹ï¼Ÿ",
                        "ä»Šæ—¥å¤©æ°”ç‚¹æ ·ï¼Ÿ",
                        "æˆ‘å“‹é£Ÿå•²ä¹œå¥½ï¼Ÿ",
                        "æ˜ŸæœŸæ—¥æœ‰å’©å¥½å»å¤„ï¼Ÿ",
                        "å¸®æˆ‘è®°ä½å˜¢"
                    ],
                    "interval_between_queries": 1.0
                },
                expected_results={
                    "all_responses_successful": True,
                    "average_response_time_ms": 2500,
                    "no_response_degradation": True,
                    "system_stability": True
                },
                weight=1.2,
                critical=False
            )
        ]

        return scenarios

    async def start_user_acceptance_test(self, user_profile: Dict[str, Any]) -> str:
        """
        å¼€å§‹ç”¨æˆ·éªŒæ”¶æµ‹è¯•

        Args:
            user_profile: ç”¨æˆ·æ¡£æ¡ˆ

        Returns:
            æµ‹è¯•ä¼šè¯ID
        """
        session_id = f"uat_{int(time.time())}_{uuid.uuid4().hex[:8]}"

        logger.info(f"ğŸš€ å¼€å§‹ç”¨æˆ·éªŒæ”¶æµ‹è¯• - ä¼šè¯ID: {session_id}")

        # åˆ›å»ºæµ‹è¯•ä¼šè¯
        self.current_session = UserAcceptanceTestSession(
            session_id=session_id,
            user_profile=user_profile,
            test_date=datetime.now(),
            scenarios=self.test_scenarios.copy(),
            status=TestStatus.RUNNING
        )

        # æ·»åŠ ç”¨æˆ·ç‰¹å®šçš„æµ‹è¯•åœºæ™¯
        await self._add_user_specific_scenarios(user_profile)

        return session_id

    async def _add_user_specific_scenarios(self, user_profile: Dict[str, Any]) -> None:
        """æ·»åŠ ç”¨æˆ·ç‰¹å®šçš„æµ‹è¯•åœºæ™¯"""
        user_age = user_profile.get("age", 30)
        user_type = user_profile.get("type", "general")
        language_preference = user_profile.get("language_preference", "cantonese")

        # æ ¹æ®ç”¨æˆ·ç”»åƒè°ƒæ•´æµ‹è¯•åœºæ™¯æƒé‡
        for scenario in self.current_session.scenarios:
            if user_type == "elderly" and scenario.category == "cultural_understanding":
                scenario.weight *= 1.5
            elif user_type == "child" and scenario.category == "accessibility":
                scenario.weight *= 1.8
            elif user_type == "family" and scenario.category == "multimodal_vision":
                scenario.weight *= 1.3

        # æ·»åŠ è¯­è¨€ç‰¹å®šæµ‹è¯•
        if language_preference == "cantonese":
            cantonese_scenario = TestScenario(
                id="cantonese_proficiency_test",
                name="ç²¤è¯­ç†Ÿç»ƒåº¦æµ‹è¯•",
                description="æµ‹è¯•ç³»ç»Ÿå¯¹ç²¤è¯­æœ¯è¯­å’Œè¡¨è¾¾çš„ç†è§£",
                category="language_specific",
                user_profile=user_type,
                test_data={
                    "input_type": "voice",
                    "content": "å‘¢ä¸ªå˜¢å¥½é¬¼æ­»æ­£ï¼ŒæŠµä½ ä¹°ï¼",
                    "context": {"language_style": "colloquial_cantonese"}
                },
                expected_results={
                    "understands_colloquial": True,
                    "appropriate_cantonese_response": True,
                    "cultural_context_understanding": True
                },
                weight=1.5,
                critical=True
            )
            self.current_session.scenarios.append(cantonese_scenario)

    async def run_test_scenario(self, scenario_id: str) -> TestResult:
        """
        è¿è¡Œæµ‹è¯•åœºæ™¯

        Args:
            scenario_id: åœºæ™¯ID

        Returns:
            æµ‹è¯•ç»“æœ
        """
        if not self.current_session:
            raise RuntimeError("æ²¡æœ‰æ´»è·ƒçš„æµ‹è¯•ä¼šè¯")

        scenario = next((s for s in self.current_session.scenarios if s.id == scenario_id), None)
        if not scenario:
            raise ValueError(f"æµ‹è¯•åœºæ™¯ä¸å­˜åœ¨: {scenario_id}")

        logger.info(f"ğŸ§ª è¿è¡Œæµ‹è¯•åœºæ™¯: {scenario.name}")

        test_result = TestResult(
            scenario_id=scenario_id,
            status=TestStatus.RUNNING,
            start_time=time.time()
        )

        try:
            # æ‰§è¡Œæµ‹è¯•
            if scenario.test_data.get("input_type") == "multimodal":
                actual_results = await self._run_multimodal_test(scenario)
            elif scenario.test_data.get("input_type") == "rapid_sequence":
                actual_results = await self._run_performance_test(scenario)
            else:
                actual_results = await self._run_single_test(scenario)

            # è¯„ä¼°ç»“æœ
            test_result.actual_results = actual_results
            test_result.success = await self._evaluate_test_results(scenario, actual_results)

            # è®°å½•å“åº”æ—¶é—´
            test_result.response_time_ms = actual_results.get("response_time_ms", 0)

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            test_result.performance_metrics = await self._calculate_performance_metrics(scenario, actual_results)

            test_result.status = TestStatus.COMPLETED
            test_result.end_time = time.time()

            logger.info(f"âœ… æµ‹è¯•åœºæ™¯å®Œæˆ: {scenario.name} - æˆåŠŸ: {test_result.success}")

        except Exception as e:
            test_result.status = TestStatus.FAILED
            test_result.error_message = str(e)
            test_result.end_time = time.time()
            logger.error(f"âŒ æµ‹è¯•åœºæ™¯å¤±è´¥: {scenario.name} - {str(e)}")

        # æ·»åŠ åˆ°ä¼šè¯ç»“æœ
        self.current_session.results.append(test_result)

        return test_result

    async def _run_single_test(self, scenario: TestScenario) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        start_time = time.time()

        try:
            # æ¨¡æ‹Ÿç³»ç»Ÿå“åº” (å®é™…åº”è¯¥è°ƒç”¨SystemIntegrationOptimizer)
            test_content = scenario.test_data.get("content", "")

            # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
            await asyncio.sleep(np.random.uniform(0.5, 2.0))

            # ç”Ÿæˆæ¨¡æ‹Ÿå“åº”
            response = self._generate_mock_response(test_content, scenario.category)

            response_time_ms = int((time.time() - start_time) * 1000)

            return {
                "response": response,
                "response_time_ms": response_time_ms,
                "success": True,
                "cantonese_response": self._is_cantonese_response(response),
                "natural_language": self._is_natural_language(response)
            }

        except Exception as e:
            return {
                "response": "",
                "response_time_ms": int((time.time() - start_time) * 1000),
                "success": False,
                "error": str(e)
            }

    async def _run_multimodal_test(self, scenario: TestScenario) -> Dict[str, Any]:
        """è¿è¡Œå¤šæ¨¡æ€æµ‹è¯•"""
        start_time = time.time()

        try:
            audio_input = scenario.test_data.get("audio", "")
            image_input = scenario.test_data.get("image", "")

            # æ¨¡æ‹Ÿå¤šæ¨¡æ€å¤„ç†
            await asyncio.sleep(np.random.uniform(1.0, 2.5))

            # ç”Ÿæˆå¤šæ¨¡æ€å“åº”
            response = f"æˆ‘ç‡åˆ°ä½ å±•ç¤ºå˜…å˜¢ï¼Œå¬åˆ°ä½ è®²'{audio_input}'ã€‚"

            response_time_ms = int((time.time() - start_time) * 1000)

            return {
                "response": response,
                "response_time_ms": response_time_ms,
                "success": True,
                "visual_understanding": True,
                "audio_understanding": True,
                "multimodal_integration": True
            }

        except Exception as e:
            return {
                "response": "",
                "response_time_ms": int((time.time() - start_time) * 1000),
                "success": False,
                "error": str(e)
            }

    async def _run_performance_test(self, scenario: TestScenario) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        queries = scenario.test_data.get("queries", [])
        interval = scenario.test_data.get("interval_between_queries", 1.0)

        results = []
        total_start_time = time.time()

        for i, query in enumerate(queries):
            query_start_time = time.time()

            try:
                # æ¨¡æ‹ŸæŸ¥è¯¢å¤„ç†
                await asyncio.sleep(np.random.uniform(0.3, 1.5))
                response = f"æ”¶åˆ°ä½ å˜…æŸ¥è¯¢: {query}"

                query_time_ms = int((time.time() - query_start_time) * 1000)
                results.append({
                    "query": query,
                    "response": response,
                    "response_time_ms": query_time_ms,
                    "success": True
                })

            except Exception as e:
                query_time_ms = int((time.time() - query_start_time) * 1000)
                results.append({
                    "query": query,
                    "response": "",
                    "response_time_ms": query_time_ms,
                    "success": False,
                    "error": str(e)
                })

            if i < len(queries) - 1:
                await asyncio.sleep(interval)

        total_time_ms = int((time.time() - total_start_time) * 1000)
        successful_results = [r for r in results if r["success"]]
        response_times = [r["response_time_ms"] for r in successful_results]

        return {
            "total_queries": len(queries),
            "successful_queries": len(successful_results),
            "success_rate": len(successful_results) / len(queries),
            "total_time_ms": total_time_ms,
            "average_response_time_ms": statistics.mean(response_times) if response_times else 0,
            "max_response_time_ms": max(response_times) if response_times else 0,
            "min_response_time_ms": min(response_times) if response_times else 0,
            "all_results": results
        }

    def _generate_mock_response(self, input_text: str, category: str) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿå“åº”"""
        # ç®€åŒ–çš„å“åº”ç”Ÿæˆé€»è¾‘
        if "æ—©æ™¨" in input_text or "ä½ å¥½" in input_text:
            return "æ—©æ™¨ï¼ä»Šæ—¥è¿‡å¾—ç‚¹æ ·å•Šï¼Ÿ"
        elif "è‹¹æœ" in input_text or "æ°´æœ" in input_text:
            return "å‘¢ä¸ªè‹¹æœç‡èµ·èº«å¥½æ–°é²œï¼è‹¹æœå¯Œå«ç»´ç”Ÿç´ Cï¼Œå¯¹èº«ä½“å¥½å¥½ã€‚"
        elif "æ™šé¤" in input_text or "ç…®é¤¸" in input_text:
            return "ä»Šæ™šå¯ä»¥è€ƒè™‘è’¸é±¼ã€ç‚’èœï¼Œå†åŠ ä¸ªæ±¤ã€‚è¿™æ ·è¥å…»å‡è¡¡ï¼Œéƒ½å¥½æ˜“å‡†å¤‡ã€‚"
        elif "ä¸­ç§‹èŠ‚" in input_text:
            return "ä¸­ç§‹èŠ‚ç³»ä¸­å›½ä¼ ç»Ÿæ–‡åŒ–èŠ‚æ—¥ï¼Œæœ‰èµæœˆã€é£Ÿæœˆé¥¼å˜…ä¼ ç»Ÿã€‚ä¸€å®¶äººä¸€é½èµæœˆæœ€æ¸©é¦¨ã€‚"
        elif "æ‰‹æœº" in input_text:
            return "ä¹°æ‰‹æœºå¯ä»¥è€ƒè™‘Samsungã€Appleæˆ–è€…åä¸ºï¼Œä¸»è¦ç‡ä½ å˜…é¢„ç®—åŒéœ€è¦ã€‚"
        elif "ç«è­¦" in input_text or "ç´§æ€¥" in input_text:
            return "å¬åˆ°æœ‰ç´§æ€¥æƒ…å†µï¼è¯·ç«‹å³ç¦»å¼€å±é™©åŒºåŸŸï¼Œæ‹¨æ‰“119æ±‚åŠ©ã€‚å®‰å…¨ç¬¬ä¸€ï¼"
        elif "é™ªæˆ‘ç©" in input_text:
            return "å¥½å‘€ï¼æˆ‘å“‹ä¸€é½ç©å•²æœ‰è¶£å˜…æ¸¸æˆå•¦ã€‚ä½ æƒ³ç©å’©æ¸¸æˆå‘¢ï¼Ÿ"
        else:
            return "æˆ‘æ˜ç™½ä½ å˜…æ„æ€ã€‚æœ‰å’©å¯ä»¥å¸®åˆ°ä½ ï¼Ÿ"

    def _is_cantonese_response(self, response: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ç²¤è¯­å“åº”"""
        cantonese_indicators = ["å‘¢", "ä¸ª", "å˜…", "å‘€", "å•¦", "å–", "å˜", "å’—", "ä¿‚", "å†‡", "ç‡", "é£Ÿ", "å˜¢"]
        return any(indicator in response for indicator in cantonese_indicators)

    def _is_natural_language(self, response: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è‡ªç„¶è¯­è¨€"""
        # ç®€åŒ–å®ç°ï¼šæ£€æŸ¥å“åº”é•¿åº¦å’Œç»“æ„
        return len(response) > 5 and not response.isupper()

    async def _evaluate_test_results(self, scenario: TestScenario, actual_results: Dict[str, Any]) -> bool:
        """è¯„ä¼°æµ‹è¯•ç»“æœ"""
        expected = scenario.expected_results
        actual = actual_results

        # åŸºç¡€æˆåŠŸæ£€æŸ¥
        if not actual.get("success", False):
            return False

        # å“åº”æ—¶é—´æ£€æŸ¥
        if "max_response_time_ms" in expected:
            if actual.get("response_time_ms", float('inf')) > expected["max_response_time_ms"]:
                return False

        # å†…å®¹æ£€æŸ¥
        for key, expected_value in expected.items():
            if key == "response_contains" and isinstance(expected_value, list):
                response = actual.get("response", "")
                if not any(term in response for term in expected_value):
                    return False
            elif key == "cantonese_response" and expected_value:
                if not actual.get("cantonese_response", False):
                    return False
            elif key == "success_rate" and isinstance(expected_value, float):
                if actual.get("success_rate", 0) < expected_value:
                    return False

        return True

    async def _calculate_performance_metrics(self, scenario: TestScenario, actual_results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            "response_time_score": 0.0,
            "accuracy_score": 0.0,
            "naturalness_score": 0.0,
            "overall_performance_score": 0.0
        }

        # å“åº”æ—¶é—´è¯„åˆ†
        response_time = actual_results.get("response_time_ms", 0)
        max_time = scenario.max_response_time_ms
        if response_time > 0:
            metrics["response_time_score"] = max(0, 1.0 - (response_time / max_time))

        # å‡†ç¡®æ€§è¯„åˆ†
        if actual_results.get("success", False):
            metrics["accuracy_score"] = 1.0

        # è‡ªç„¶åº¦è¯„åˆ†
        if actual_results.get("natural_language", False) and actual_results.get("cantonese_response", False):
            metrics["naturalness_score"] = 1.0
        elif actual_results.get("natural_language", False):
            metrics["naturalness_score"] = 0.7

        # ç»¼åˆæ€§èƒ½è¯„åˆ†
        metrics["overall_performance_score"] = (
            metrics["response_time_score"] * 0.4 +
            metrics["accuracy_score"] * 0.4 +
            metrics["naturalness_score"] * 0.2
        )

        return metrics

    async def complete_user_acceptance_test(self, user_feedback: Optional[str] = None) -> Dict[str, Any]:
        """
        å®Œæˆç”¨æˆ·éªŒæ”¶æµ‹è¯•

        Args:
            user_feedback: ç”¨æˆ·åé¦ˆ

        Returns:
            æµ‹è¯•æ€»ç»“æŠ¥å‘Š
        """
        if not self.current_session:
            raise RuntimeError("æ²¡æœ‰æ´»è·ƒçš„æµ‹è¯•ä¼šè¯")

        logger.info("ğŸ“Š å®Œæˆç”¨æˆ·éªŒæ”¶æµ‹è¯•ï¼Œç”Ÿæˆæ€»ç»“æŠ¥å‘Š")

        # è®¡ç®—æ€»ä½“è¯„åˆ†
        overall_score = await self._calculate_overall_score()
        self.current_session.overall_score = overall_score

        # ç¡®å®šç”¨æˆ·æ»¡æ„åº¦
        self.current_session.user_satisfaction = self._determine_satisfaction_level(overall_score)

        # ç”Ÿæˆå»ºè®®
        recommendations = await self._generate_recommendations()
        self.current_session.recommendations = recommendations

        # è®¾ç½®ç”¨æˆ·åé¦ˆ
        if user_feedback:
            self.current_session.user_feedback = user_feedback

        # æ›´æ–°çŠ¶æ€
        self.current_session.status = TestStatus.COMPLETED

        # æ·»åŠ åˆ°å†å²è®°å½•
        self.test_history.append(self.current_session)

        # ç”ŸæˆæŠ¥å‘Š
        report = await self._generate_test_report()

        # æ¸…ç†å½“å‰ä¼šè¯
        self.current_session = None

        return report

    async def _calculate_overall_score(self) -> float:
        """è®¡ç®—æ€»ä½“è¯„åˆ†"""
        if not self.current_session or not self.current_session.results:
            return 0.0

        total_weighted_score = 0.0
        total_weight = 0.0

        for result in self.current_session.results:
            scenario = next((s for s in self.current_session.scenarios if s.id == result.scenario_id), None)
            if not scenario:
                continue

            weight = scenario.weight
            performance_metrics = result.performance_metrics

            # è®¡ç®—åœºæ™¯è¯„åˆ†
            scenario_score = 0.0
            if result.success:
                scenario_score = performance_metrics.get("overall_performance_score", 0.0)
            else:
                scenario_score = 0.0

            total_weighted_score += scenario_score * weight
            total_weight += weight

        return total_weighted_score / total_weight if total_weight > 0 else 0.0

    def _determine_satisfaction_level(self, score: float) -> UserSatisfactionLevel:
        """ç¡®å®šæ»¡æ„åº¦ç­‰çº§"""
        if score >= 0.9:
            return UserSatisfactionLevel.EXCELLENT
        elif score >= 0.8:
            return UserSatisfactionLevel.GOOD
        elif score >= 0.6:
            return UserSatisfactionLevel.SATISFACTORY
        elif score >= 0.4:
            return UserSatisfactionLevel.POOR
        else:
            return UserSatisfactionLevel.VERY_POOR

    async def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        if not self.current_session or not self.current_session.results:
            return recommendations

        # åˆ†æå¤±è´¥çš„æµ‹è¯•åœºæ™¯
        failed_scenarios = [r for r in self.current_session.results if not r.success]
        slow_scenarios = [r for r in self.current_session.results
                         if r.response_time_ms and r.response_time_ms > self.test_config["max_response_time_ms"]]

        # ç”Ÿæˆå»ºè®®
        if failed_scenarios:
            recommendations.append("å»ºè®®ä¼˜åŒ–è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡ï¼Œç‰¹åˆ«æ˜¯å¯¹ç²¤è¯­å£è¯­çš„ç†è§£")
            recommendations.append("åŠ å¼ºå¤šæ¨¡æ€é›†æˆï¼Œæå‡è§†è§‰+è¯­éŸ³ååŒå¤„ç†èƒ½åŠ›")

        if slow_scenarios:
            recommendations.append("ä¼˜åŒ–ç³»ç»Ÿå“åº”æ—¶é—´ï¼Œç›®æ ‡æ§åˆ¶åœ¨3ç§’ä»¥å†…")
            recommendations.append("è€ƒè™‘å¢åŠ ç¼“å­˜æœºåˆ¶ï¼Œæå‡é‡å¤æŸ¥è¯¢çš„å“åº”é€Ÿåº¦")

        # æ£€æŸ¥ç²¤è¯­è‡ªç„¶åº¦
        cantonese_issues = [r for r in self.current_session.results
                           if not r.actual_results.get("cantonese_response", False)]
        if cantonese_issues:
            recommendations.append("æ”¹è¿›ç²¤è¯­ç”Ÿæˆæ¨¡å‹ï¼Œæä¾›æ›´è‡ªç„¶çš„ç²¤è¯­è¡¨è¾¾")
            recommendations.append("å¢åŠ æ›´å¤šç²¤è¯­æ–‡åŒ–å…ƒç´ å’Œæœ¬åœ°åŒ–å†…å®¹")

        # æ€§èƒ½å»ºè®®
        avg_response_time = statistics.mean([r.response_time_ms for r in self.current_session.results
                                           if r.response_time_ms]) if self.current_session.results else 0
        if avg_response_time > 2500:
            recommendations.append("è€ƒè™‘ä¼˜åŒ–ç½‘ç»œè¿æ¥å’ŒAPIè°ƒç”¨æ•ˆç‡")
            recommendations.append("å®ç°æ›´æ™ºèƒ½çš„é¢„æµ‹ç¼“å­˜å’Œé¢„åŠ è½½æœºåˆ¶")

        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¡¨ç°ä¼˜ç§€ï¼Œå»ºè®®æŒç»­ç›‘æ§å¹¶æ”¶é›†æ›´å¤šç”¨æˆ·åé¦ˆ")

        return recommendations

    async def _generate_test_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if not self.current_session:
            return {"error": "æ²¡æœ‰æµ‹è¯•æ•°æ®"}

        session = self.current_session
        results = session.results

        # ç»Ÿè®¡ä¿¡æ¯
        total_scenarios = len(results)
        successful_scenarios = len([r for r in results if r.success])
        success_rate = successful_scenarios / total_scenarios if total_scenarios > 0 else 0

        response_times = [r.response_time_ms for r in results if r.response_time_ms]
        avg_response_time = statistics.mean(response_times) if response_times else 0

        # åˆ†ç±»ç»Ÿè®¡
        category_stats = {}
        for scenario in session.scenarios:
            category = scenario.category
            if category not in category_stats:
                category_stats[category] = {"total": 0, "successful": 0}

            category_stats[category]["total"] += 1

            result = next((r for r in results if r.scenario_id == scenario.id), None)
            if result and result.success:
                category_stats[category]["successful"] += 1

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "session_info": {
                "session_id": session.session_id,
                "test_date": session.test_date.isoformat(),
                "user_profile": session.user_profile
            },
            "summary": {
                "total_scenarios": total_scenarios,
                "successful_scenarios": successful_scenarios,
                "success_rate": success_rate,
                "overall_score": session.overall_score,
                "user_satisfaction": session.user_satisfaction.value,
                "average_response_time_ms": avg_response_time,
                "status": session.status.value
            },
            "category_performance": category_stats,
            "detailed_results": [asdict(result) for result in results],
            "recommendations": session.recommendations,
            "user_feedback": session.user_feedback,
            "performance_baselines": self.performance_baselines,
            "generated_at": datetime.now().isoformat()
        }

        return report

    def get_test_session_status(self, session_id: Optional[str] = None) -> Dict[str, Any]:
        """è·å–æµ‹è¯•ä¼šè¯çŠ¶æ€"""
        if session_id:
            # æŸ¥æ‰¾ç‰¹å®šä¼šè¯
            session = next((s for s in self.test_history if s.session_id == session_id), None)
            if session:
                return asdict(session)
            else:
                return {"error": "ä¼šè¯ä¸å­˜åœ¨"}
        elif self.current_session:
            # è¿”å›å½“å‰ä¼šè¯
            return asdict(self.current_session)
        else:
            return {"status": "no_active_session"}

    def get_test_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–æµ‹è¯•å†å²"""
        history = self.test_history[-limit:] if limit > 0 else self.test_history
        return [asdict(session) for session in history]

# å·¥å‚å‡½æ•°
def create_cantonese_uat() -> CantoneseUserAcceptanceTest:
    """åˆ›å»ºç²¤è¯­å®¶åº­ç”¨æˆ·éªŒæ”¶æµ‹è¯•å®ä¾‹"""
    return CantoneseUserAcceptanceTest()