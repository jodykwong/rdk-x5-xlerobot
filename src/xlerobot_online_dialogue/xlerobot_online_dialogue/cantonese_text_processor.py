"""
CantoneseTextProcessor - ç²¤è¯­æ–‡æœ¬é¢„å¤„ç†
Story 1.7: å¤šæ¨¡æ€åœ¨çº¿å¯¹è¯APIé›†æˆ
ä¸¥æ ¼éµå¾ªEpic 1çº¯åœ¨çº¿æ¶æ„ - ä»…æ–‡æœ¬é¢„å¤„ç†ï¼Œæ— æœ¬åœ°å¯¹è¯é€»è¾‘
"""

import re
from typing import Dict, List
import logging

logger = logging.getLogger(__name__)

class CantoneseTextProcessor:
    """
    ç²¤è¯­æ–‡æœ¬é¢„å¤„ç†å™¨
    ä¸¥æ ¼éµå¾ªEpic 1çº¯åœ¨çº¿æ¶æ„ - ä»…æ–‡æœ¬é¢„å¤„ç†ï¼Œæ— æœ¬åœ°å¯¹è¯é€»è¾‘
    ç”¨äºåœ¨APIè°ƒç”¨å‰ä¼˜åŒ–ç²¤è¯­æ–‡æœ¬è¾“å…¥
    """

    def __init__(self):
        """åˆå§‹åŒ–ç²¤è¯­æ–‡æœ¬é¢„å¤„ç†å™¨"""
        logger.info("ğŸŒ åˆå§‹åŒ–CantoneseTextProcessor - çº¯åœ¨çº¿æ¶æ„")

        # ç²¤è¯­æ ‡å‡†æœ¯è¯­æ˜ å°„
        self.standard_terms = {
            # åŸºç¡€è¯æ±‡
            "æˆ‘": "æˆ‘",
            "ä½ ": "ä½ ",
            "ä»–": "ä½¢",
            "å¥¹": "ä½¢",
            "æˆ‘ä»¬": "æˆ‘å“‹",
            "ä½ ä»¬": "ä½ å“‹",
            "ä»–ä»¬": "ä½¢å“‹",

            # å¸¸ç”¨è¡¨è¾¾
            "ä»€ä¹ˆ": "ä¹œå˜¢",
            "æ€ä¹ˆ": "é»",
            "ä¸ºä»€ä¹ˆ": "é»è§£",
            "å“ªé‡Œ": "é‚Šåº¦",
            "å“ªä¸ª": "é‚Šå€‹",
            "è¿™ä¸ª": "å‘¢å€‹",
            "é‚£ä¸ª": "å—°å€‹",
            "è¿™é‡Œ": "å‘¢åº¦",
            "é‚£é‡Œ": "å—°åº¦",

            # åŠ¨è¯
            "æ˜¯": "ä¿‚",
            "æœ‰": "æœ‰",
            "æ²¡æœ‰": "å†‡",
            "ä¸æ˜¯": "å””ä¿‚",
            "ä¼š": "æœƒ",
            "å¯ä»¥": "å¯ä»¥",
            "æƒ³è¦": "æƒ³",
            "éœ€è¦": "éœ€è¦",
            "çŸ¥é“": "çŸ¥",

            # å½¢å®¹è¯
            "å¥½": "å¥½",
            "ä¸å¥½": "å””å¥½",
            "å¯¹çš„": "å•±å˜…",
            "é”™çš„": "éŒ¯å˜…",
            "æ¼‚äº®": "éš",
            "å¥½çœ‹": "å¥½ç‡",
            "å¥½åƒ": "å¥½é£Ÿ",
            "å‰å®³": "çŠ€åˆ©",
            "å‰å®³": "å»",

            # åŠ©è¯
            "çš„": "å˜…",
            "äº†": "å–‡",
            "å—": "å˜›",
            "å‘¢": "å‘¢",
            "å•Š": "å‘€",
            "å§": "å•¦",

            # ç¤¼è²Œç”¨è¯­
            "è°¢è°¢": "å””è©²",
            "è¯·": "å””è©²",
            "å¯¹ä¸èµ·": "å°å””ä½",
            "ä¸å¥½æ„æ€": "å””å¥½æ„æ€",
            "å†è§": "æ‹œæ‹œ",
            "ä½ å¥½": "ä½ å¥½",
            "æ—©å®‰": "æ—©æ™¨",
            "æ™šå®‰": "å¤œæ™šå¥½",

            # æ—¶é—´è¡¨è¾¾
            "ç°åœ¨": "è€Œå®¶",
            "ç„¶å": "ç„¶å¾Œ",
            "åæ¥": "å¾ŒåšŸ",
            "é©¬ä¸Š": "å³åˆ»",
            "ç­‰ä¸€ä¸‹": "ç­‰ä¸€ç­‰",
            "ä¸€ä¼šå„¿": "ä¸€é™£é–“",

            # æ•°é‡è¯
            "ä¸€ä¸ª": "ä¸€å€‹",
            "ä¸€äº›": "å•²",
            "æ‰€æœ‰": "æ‰€æœ‰",
            "å¾ˆå¤š": "å¥½å¤š",
            "å¾ˆå°‘": "å¥½å°‘",
            "æ²¡æœ‰": "å†‡",

            # ç–‘é—®è¯
            "å—": "å˜›",
            "å‘¢": "å‘¢",
            "å§": "å‘€",
            "å‘¢": "å‘¢",
            "å—": "å˜›",
            "å‘€": "å‘€",
        }

        # ç²¤è¯­ç‰¹æ®Šè¡¨è¾¾
        self.special_expressions = {
            # é—®å¥æ¨¡å¼
            "æ˜¯å—": "ä¿‚å˜›",
            "å¯¹å—": "å•±å˜›",
            "ä½ çŸ¥é“å—": "ä½ çŸ¥å””çŸ¥",
            "æ˜ç™½å—": "æ˜å””æ˜",
            "å¯ä»¥å—": "å¯ä»¥å‘€",

            # å›åº”æ¨¡å¼
            "å¥½çš„": "å¥½å˜…",
            "æ˜¯çš„": "ä¿‚å˜…",
            "å½“ç„¶": "ç•¶ç„¶",
            "æ²¡é”™": "å†‡éŒ¯",
            "æ˜¯çš„å‘¢": "ä¿‚å‘€",
            "æ²¡é—®é¢˜": "å†‡å•é¡Œ",

            # æƒ…æ„Ÿè¡¨è¾¾
            "å¾ˆå¼€å¿ƒ": "å¥½é–‹å¿ƒ",
            "å¾ˆç”Ÿæ°”": "å¥½å¬²",
            "å¾ˆå¤±æœ›": "å¥½å¤±æœ›",
            "å¾ˆæƒŠè®¶": "å¥½é©šè¨",
            "å¾ˆæ‹…å¿ƒ": "å¥½æ“”å¿ƒ",

            # ç”Ÿæ´»å¸¸ç”¨
            "åƒé¥­": "é£Ÿé£¯",
            "ç¡è§‰": "ç“è¦º",
            "æ´—æ¾¡": "æ²–æ¶¼",
            "ä¸Šç­": "è¿”å·¥",
            "ä¸‹ç­": "æ”¾å·¥",
            "å›å®¶": "è¿”å±‹ä¼",
            "å‡ºé—¨": "å‡ºé–€",
            "é€›è¡—": "è¡Œè¡—",
            "è´­ç‰©": "è²·å˜¢",
        }

        # éœ€è¦ä¿ç•™çš„ä¸“æœ‰åè¯ï¼ˆä¸è½¬æ¢ï¼‰
        self.proper_nouns = {
            "é˜¿é‡Œäº‘", "è…¾è®¯", "åä¸º", "å°ç±³",
            "XleRobot", "Robot", "AI", "API",
            "Python", "JavaScript", "Java", "C++",
            "Windows", "Linux", "Android", "iOS",
        }

        # é¢„å¤„ç†ç»Ÿè®¡
        self.preprocessing_stats = {
            "total_processed": 0,
            "terms_converted": 0,
            "expressions_converted": 0,
            "length_reduced": 0
        }

        logger.info("âœ… ç²¤è¯­é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def preprocess_text(self, text: str) -> str:
        """
        é¢„å¤„ç†ç²¤è¯­æ–‡æœ¬
        ä»…è¿›è¡Œç®€å•çš„æœ¯è¯­è½¬æ¢ï¼Œä¸è¿›è¡Œä»»ä½•æœ¬åœ°å¯¹è¯é€»è¾‘

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            str: é¢„å¤„ç†åçš„æ–‡æœ¬
        """
        if not text:
            return text

        self.preprocessing_stats["total_processed"] += 1
        original_length = len(text)

        processed_text = text

        # 1. æ ‡å‡†æœ¯è¯­è½¬æ¢
        processed_text = self._convert_standard_terms(processed_text)

        # 2. ç‰¹æ®Šè¡¨è¾¾è½¬æ¢
        processed_text = self._convert_special_expressions(processed_text)

        # 3. æ¸…ç†å¤šä½™ç©ºæ ¼
        processed_text = self._clean_whitespace(processed_text)

        # 4. é•¿åº¦æ£€æŸ¥ï¼ˆAPIé™åˆ¶ï¼‰
        processed_text = self._check_length_limit(processed_text)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.preprocessing_stats["length_reduced"] += original_length - len(processed_text)

        logger.debug(f"ğŸ“ æ–‡æœ¬é¢„å¤„ç†: {text[:20]}... â†’ {processed_text[:20]}...")

        return processed_text

    def _convert_standard_terms(self, text: str) -> str:
        """
        è½¬æ¢æ ‡å‡†ç²¤è¯­æœ¯è¯­
        """
        converted_text = text
        converted_count = 0

        for standard, cantonese in self.standard_terms.items():
            if standard in converted_text:
                converted_text = converted_text.replace(standard, cantonese)
                converted_count += 1

        self.preprocessing_stats["terms_converted"] += converted_count
        return converted_text

    def _convert_special_expressions(self, text: str) -> str:
        """
        è½¬æ¢ç‰¹æ®Šç²¤è¯­è¡¨è¾¾
        """
        converted_text = text
        converted_count = 0

        for expression, cantonese in self.special_expressions.items():
            if expression in converted_text:
                converted_text = converted_text.replace(expression, cantonese)
                converted_count += 1

        self.preprocessing_stats["expressions_converted"] += converted_count
        return converted_text

    def _clean_whitespace(self, text: str) -> str:
        """
        æ¸…ç†å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
        """
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text.strip())

        # æ¸…ç†å¤šä½™æ ‡ç‚¹
        text = re.sub(r'[ã€‚ï¼Œï¼ï¼Ÿ]{2,}', lambda m: m.group(0)[0], text)

        return text

    def _check_length_limit(self, text: str) -> str:
        """
        æ£€æŸ¥æ–‡æœ¬é•¿åº¦é™åˆ¶ï¼ˆAPIè¦æ±‚ï¼‰
        å¤§å¤šæ•°APIé™åˆ¶åœ¨2000å­—ç¬¦ä»¥å†…
        """
        max_length = 2000

        if len(text) > max_length:
            truncated = text[:max_length - 3] + "..."
            logger.warning(f"âš ï¸ æ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­: {len(text)} â†’ {len(truncated)}")
            return truncated

        return text

    def get_preprocessing_statistics(self) -> Dict[str, any]:
        """
        è·å–é¢„å¤„ç†ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict[str, any]: ç»Ÿè®¡ä¿¡æ¯
        """
        return self.preprocessing_stats.copy()

    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.preprocessing_stats = {
            "total_processed": 0,
            "terms_converted": 0,
            "expressions_converted": 0,
            "length_reduced": 0
        }
        logger.info("ğŸ“Š é¢„å¤„ç†ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")

    def validate_cantonese_text(self, text: str) -> Dict[str, bool]:
        """
        éªŒè¯ç²¤è¯­æ–‡æœ¬è´¨é‡

        Args:
            text: å¾…éªŒè¯æ–‡æœ¬

        Returns:
            Dict[str, bool]: éªŒè¯ç»“æœ
        """
        return {
            "has_cantonese_characters": bool(re.search(r'[ç²¤èªç¹ç®€]', text)),
            "uses_cantonese_terms": any(term in text for term in self.standard_terms.values()),
            "appropriate_length": 10 <= len(text) <= 500,
            "no_consecutive_punctuation": not re.search(r'[ã€‚ï¼Œï¼ï¼Ÿ]{2,}', text),
            "proper_terminology": not any(pnoun in text for pnoun in self.proper_nouns.keys() if pnoun in text.lower())
        }

# å…¨å±€é¢„å¤„ç†å™¨å®ä¾‹
_global_preprocessor = None

def get_cantonese_preprocessor() -> CantoneseTextProcessor:
    """
    è·å–å…¨å±€ç²¤è¯­é¢„å¤„ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    """
    global _global_preprocessor

    if _global_preprocessor is None:
        _global_preprocessor = CantoneseTextProcessor()

    return _global_preprocessor