#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
XleRobot Story 1.4 - TTSæ€§èƒ½æµ‹è¯•è„šæœ¬
BMad-Method v6 Brownfield Level 4 ä¼ä¸šçº§å®ç°
Story 1.4: åŸºç¡€è¯­éŸ³åˆæˆ (é˜¿é‡Œäº‘TTS APIé›†æˆ)

å…¨é¢æµ‹è¯•TTSæœåŠ¡çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å“åº”æ—¶é—´ã€èµ„æºä½¿ç”¨ã€å¹¶å‘èƒ½åŠ›ç­‰
"""

import os
import sys
import time
import threading
import statistics
import json
import logging
import psutil
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any
from dataclasses import dataclass, asdict
import tempfile

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'src'))

from xlerobot.tts.aliyun_tts_client import AliyunTTSClient
from xlerobot.tts.audio_processor import AudioProcessor


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    test_name: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    p95_response_time: float
    p99_response_time: float
    requests_per_second: float
    memory_usage_mb: float
    cpu_usage_percent: float
    audio_quality_avg: float
    error_rate: float

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


class TTSPerformanceTester:
    """TTSæ€§èƒ½æµ‹è¯•å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•å™¨"""
        self.setup_logging()
        self.client = None
        self.processor = None
        self.process = psutil.Process()

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def initialize_components(self):
        """åˆå§‹åŒ–TTSç»„ä»¶"""
        try:
            from xlerobot.tts.audio_processor import TTSConfigManager
            config_manager = TTSConfigManager()
            config = config_manager.get_config()

            self.client = AliyunTTSClient(config)
            self.processor = AudioProcessor()

            self.logger.info("âœ… TTSç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            return True
        except Exception as e:
            self.logger.error(f"âŒ TTSç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def get_system_metrics(self) -> Dict[str, float]:
        """è·å–ç³»ç»ŸæŒ‡æ ‡"""
        try:
            memory_info = self.process.memory_info()
            return {
                'memory_mb': memory_info.rss / 1024 / 1024,
                'cpu_percent': self.process.cpu_percent(),
                'threads': self.process.num_threads()
            }
        except Exception as e:
            self.logger.error(f"è·å–ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")
            return {'memory_mb': 0, 'cpu_percent': 0, 'threads': 0}

    def single_request_test(self, text: str, **kwargs) -> Dict[str, Any]:
        """å•ä¸ªè¯·æ±‚æµ‹è¯•"""
        result = {
            'success': False,
            'response_time': 0,
            'audio_size': 0,
            'quality_score': 0,
            'error': None
        }

        try:
            start_time = time.time()
            audio_data = self.client.synthesize_speech(text, **kwargs)
            end_time = time.time()

            if audio_data:
                result['success'] = True
                result['response_time'] = end_time - start_time
                result['audio_size'] = len(audio_data)

                # è¯„ä¼°éŸ³é¢‘è´¨é‡
                if self.processor:
                    quality = self.processor.evaluate_audio_quality(audio_data)
                    result['quality_score'] = quality.get('quality_score', 0)
            else:
                result['error'] = "No audio data returned"

        except Exception as e:
            result['error'] = str(e)

        return result

    def test_basic_performance(self, request_count: int = 20) -> PerformanceMetrics:
        """åŸºç¡€æ€§èƒ½æµ‹è¯•"""
        self.logger.info(f"ğŸš€ å¼€å§‹åŸºç¡€æ€§èƒ½æµ‹è¯• ({request_count} ä¸ªè¯·æ±‚)")

        results = []
        start_test_time = time.time()
        initial_metrics = self.get_system_metrics()

        test_text = "è¿™æ˜¯ä¸€ä¸ªåŸºç¡€æ€§èƒ½æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°TTSæœåŠ¡çš„å“åº”æ—¶é—´å’ŒéŸ³é¢‘è´¨é‡ã€‚"

        for i in range(request_count):
            # æ·»åŠ ä¸€äº›å˜åŒ–é¿å…ç¼“å­˜å½±å“
            text = f"{test_text} (æµ‹è¯• {i+1}/{request_count})"
            result = self.single_request_test(text)
            results.append(result)

            if (i + 1) % 5 == 0:
                self.logger.info(f"å·²å®Œæˆ {i+1}/{request_count} ä¸ªè¯·æ±‚")

        end_test_time = time.time()
        final_metrics = self.get_system_metrics()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        successful_results = [r for r in results if r['success']]
        response_times = [r['response_time'] for r in successful_results]
        quality_scores = [r['quality_score'] for r in successful_results if r['quality_score'] > 0]

        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else max(response_times)
            p99_response_time = statistics.quantiles(response_times, n=100)[98] if len(response_times) > 100 else max(response_times)
        else:
            avg_response_time = min_response_time = max_response_time = p95_response_time = p99_response_time = 0

        if quality_scores:
            audio_quality_avg = statistics.mean(quality_scores)
        else:
            audio_quality_avg = 0

        total_test_time = end_test_time - start_test_time
        requests_per_second = len(successful_results) / total_test_time if total_test_time > 0 else 0

        metrics = PerformanceMetrics(
            test_name="åŸºç¡€æ€§èƒ½æµ‹è¯•",
            total_requests=request_count,
            successful_requests=len(successful_results),
            failed_requests=request_count - len(successful_results),
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            requests_per_second=requests_per_second,
            memory_usage_mb=final_metrics['memory_mb'] - initial_metrics['memory_mb'],
            cpu_usage_percent=final_metrics['cpu_percent'],
            audio_quality_avg=audio_quality_avg,
            error_rate=(request_count - len(successful_results)) / request_count
        )

        self.logger.info(f"âœ… åŸºç¡€æ€§èƒ½æµ‹è¯•å®Œæˆ: {len(successful_results)}/{request_count} æˆåŠŸ")
        return metrics

    def test_concurrent_performance(self, thread_count: int = 5, requests_per_thread: int = 10) -> PerformanceMetrics:
        """å¹¶å‘æ€§èƒ½æµ‹è¯•"""
        self.logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘æ€§èƒ½æµ‹è¯• ({thread_count} çº¿ç¨‹, æ¯çº¿ç¨‹ {requests_per_thread} è¯·æ±‚)")

        results = []
        start_test_time = time.time()
        initial_metrics = self.get_system_metrics()

        def worker_task(thread_id: int) -> List[Dict[str, Any]]:
            """å·¥ä½œçº¿ç¨‹ä»»åŠ¡"""
            thread_results = []
            test_text = f"è¿™æ˜¯ä¸€ä¸ªå¹¶å‘æ€§èƒ½æµ‹è¯•ï¼Œçº¿ç¨‹ {thread_id} æ­£åœ¨æ‰§è¡Œè¯·æ±‚ã€‚"

            for i in range(requests_per_thread):
                text = f"{test_text} (è¯·æ±‚ {i+1}/{requests_per_thread})"
                result = self.single_request_test(text)
                result['thread_id'] = thread_id
                thread_results.append(result)

            return thread_results

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘æµ‹è¯•
        with ThreadPoolExecutor(max_workers=thread_count) as executor:
            futures = [executor.submit(worker_task, i) for i in range(thread_count)]

            for future in as_completed(futures):
                try:
                    thread_results = future.result()
                    results.extend(thread_results)
                except Exception as e:
                    self.logger.error(f"çº¿ç¨‹æ‰§è¡Œå¤±è´¥: {e}")

        end_test_time = time.time()
        final_metrics = self.get_system_metrics()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡ (ä¸åŸºç¡€æµ‹è¯•ç›¸åŒ)
        successful_results = [r for r in results if r['success']]
        response_times = [r['response_time'] for r in successful_results]
        quality_scores = [r['quality_score'] for r in successful_results if r['quality_score'] > 0]

        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else max(response_times)
            p99_response_time = statistics.quantiles(response_times, n=100)[98] if len(response_times) > 100 else max(response_times)
        else:
            avg_response_time = min_response_time = max_response_time = p95_response_time = p99_response_time = 0

        if quality_scores:
            audio_quality_avg = statistics.mean(quality_scores)
        else:
            audio_quality_avg = 0

        total_test_time = end_test_time - start_test_time
        requests_per_second = len(successful_results) / total_test_time if total_test_time > 0 else 0

        metrics = PerformanceMetrics(
            test_name=f"å¹¶å‘æ€§èƒ½æµ‹è¯•({thread_count}çº¿ç¨‹)",
            total_requests=thread_count * requests_per_thread,
            successful_requests=len(successful_results),
            failed_requests=len(results) - len(successful_results),
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            requests_per_second=requests_per_second,
            memory_usage_mb=final_metrics['memory_mb'] - initial_metrics['memory_mb'],
            cpu_usage_percent=final_metrics['cpu_percent'],
            audio_quality_avg=audio_quality_avg,
            error_rate=(len(results) - len(successful_results)) / len(results)
        )

        self.logger.info(f"âœ… å¹¶å‘æ€§èƒ½æµ‹è¯•å®Œæˆ: {len(successful_results)}/{len(results)} æˆåŠŸ")
        return metrics

    def test_parameter_performance(self) -> PerformanceMetrics:
        """å‚æ•°è°ƒèŠ‚æ€§èƒ½æµ‹è¯•"""
        self.logger.info("ğŸš€ å¼€å§‹å‚æ•°è°ƒèŠ‚æ€§èƒ½æµ‹è¯•")

        test_cases = [
            {"name": "æ ‡å‡†è¯­éŸ³", "params": {}},
            {"name": "å¿«é€Ÿè¯­éŸ³", "params": {"speech_rate": 80}},
            {"name": "æ…¢é€Ÿè¯­éŸ³", "params": {"speech_rate": 20}},
            {"name": "é«˜éŸ³è°ƒ", "params": {"pitch_rate": 80}},
            {"name": "ä½éŸ³è°ƒ", "params": {"pitch_rate": 20}},
            {"name": "å¤§å£°è¯­éŸ³", "params": {"volume": 100}},
            {"name": "æƒ…æ„Ÿå‹å¥½", "params": {"emotion": "friendly"}},
            {"name": "æƒ…æ„Ÿç¡®è®¤", "params": {"emotion": "confirm"}},
            {"name": "è´¨é‡å¢å¼º", "params": {"enhance_quality": True}},
        ]

        results = []
        start_test_time = time.time()
        initial_metrics = self.get_system_metrics()

        for case in test_cases:
            self.logger.info(f"æµ‹è¯• {case['name']}...")
            case_results = []

            for i in range(5):  # æ¯ç§å‚æ•°æµ‹è¯•5æ¬¡
                text = f"è¿™æ˜¯{case['name']}çš„æ€§èƒ½æµ‹è¯•ï¼Œæ­£åœ¨æ‰§è¡Œç¬¬ {i+1} æ¬¡æµ‹è¯•ã€‚"

                start_time = time.time()
                audio_data = self.client.synthesize_speech(text, **case['params'])
                end_time = time.time()

                if audio_data:
                    # åº”ç”¨é¢å¤–çš„éŸ³é¢‘å¤„ç†
                    if 'emotion' in case['params']:
                        audio_data = self.processor.apply_emotion_style(audio_data, case['params']['emotion'])
                    if case['params'].get('enhance_quality'):
                        audio_data = self.processor.enhance_audio_quality(audio_data)

                    # è¯„ä¼°è´¨é‡
                    quality = self.processor.evaluate_audio_quality(audio_data)

                    result = {
                        'success': True,
                        'response_time': end_time - start_time,
                        'audio_size': len(audio_data),
                        'quality_score': quality.get('quality_score', 0),
                        'case_name': case['name']
                    }
                else:
                    result = {
                        'success': False,
                        'response_time': end_time - start_time,
                        'audio_size': 0,
                        'quality_score': 0,
                        'case_name': case['name']
                    }

                case_results.append(result)

            results.extend(case_results)

        end_test_time = time.time()
        final_metrics = self.get_system_metrics()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        successful_results = [r for r in results if r['success']]
        response_times = [r['response_time'] for r in successful_results]
        quality_scores = [r['quality_score'] for r in successful_results if r['quality_score'] > 0]

        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else max(response_times)
            p99_response_time = statistics.quantiles(response_times, n=100)[98] if len(response_times) > 100 else max(response_times)
        else:
            avg_response_time = min_response_time = max_response_time = p95_response_time = p99_response_time = 0

        if quality_scores:
            audio_quality_avg = statistics.mean(quality_scores)
        else:
            audio_quality_avg = 0

        total_test_time = end_test_time - start_test_time
        requests_per_second = len(successful_results) / total_test_time if total_test_time > 0 else 0

        metrics = PerformanceMetrics(
            test_name="å‚æ•°è°ƒèŠ‚æ€§èƒ½æµ‹è¯•",
            total_requests=len(results),
            successful_requests=len(successful_results),
            failed_requests=len(results) - len(successful_results),
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            requests_per_second=requests_per_second,
            memory_usage_mb=final_metrics['memory_mb'] - initial_metrics['memory_mb'],
            cpu_usage_percent=final_metrics['cpu_percent'],
            audio_quality_avg=audio_quality_avg,
            error_rate=(len(results) - len(successful_results)) / len(results)
        )

        self.logger.info(f"âœ… å‚æ•°è°ƒèŠ‚æ€§èƒ½æµ‹è¯•å®Œæˆ: {len(successful_results)}/{len(results)} æˆåŠŸ")
        return metrics

    def test_stability_performance(self, duration_minutes: int = 2) -> PerformanceMetrics:
        """ç¨³å®šæ€§æ€§èƒ½æµ‹è¯•"""
        self.logger.info(f"ğŸš€ å¼€å§‹ç¨³å®šæ€§æ€§èƒ½æµ‹è¯• (æŒç»­ {duration_minutes} åˆ†é’Ÿ)")

        results = []
        start_test_time = time.time()
        initial_metrics = self.get_system_metrics()
        end_time = start_test_time + (duration_minutes * 60)
        request_count = 0

        while time.time() < end_time:
            text = f"ç¨³å®šæ€§æµ‹è¯•è¯·æ±‚ {request_count + 1}ï¼Œå½“å‰æ—¶é—´: {time.strftime('%H:%M:%S')}"
            result = self.single_request_test(text)
            result['request_id'] = request_count
            results.append(result)
            request_count += 1

            # æ¯10ä¸ªè¯·æ±‚è®°å½•ä¸€æ¬¡ç³»ç»ŸæŒ‡æ ‡
            if request_count % 10 == 0:
                current_metrics = self.get_system_metrics()
                memory_growth = current_metrics['memory_mb'] - initial_metrics['memory_mb']
                self.logger.info(f"å·²å®Œæˆ {request_count} ä¸ªè¯·æ±‚, å†…å­˜å¢é•¿: {memory_growth:.2f}MB")

            # è¯·æ±‚é—´éš”
            time.sleep(0.5)

        actual_duration = time.time() - start_test_time
        final_metrics = self.get_system_metrics()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        successful_results = [r for r in results if r['success']]
        response_times = [r['response_time'] for r in successful_results]
        quality_scores = [r['quality_score'] for r in successful_results if r['quality_score'] > 0]

        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else max(response_times)
            p99_response_time = statistics.quantiles(response_times, n=100)[98] if len(response_times) > 100 else max(response_times)
        else:
            avg_response_time = min_response_time = max_response_time = p95_response_time = p99_response_time = 0

        if quality_scores:
            audio_quality_avg = statistics.mean(quality_scores)
        else:
            audio_quality_avg = 0

        requests_per_second = len(successful_results) / actual_duration if actual_duration > 0 else 0

        metrics = PerformanceMetrics(
            test_name=f"ç¨³å®šæ€§æ€§èƒ½æµ‹è¯•({duration_minutes}åˆ†é’Ÿ)",
            total_requests=request_count,
            successful_requests=len(successful_results),
            failed_requests=request_count - len(successful_results),
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            requests_per_second=requests_per_second,
            memory_usage_mb=final_metrics['memory_mb'] - initial_metrics['memory_mb'],
            cpu_usage_percent=final_metrics['cpu_percent'],
            audio_quality_avg=audio_quality_avg,
            error_rate=(request_count - len(successful_results)) / request_count
        )

        self.logger.info(f"âœ… ç¨³å®šæ€§æ€§èƒ½æµ‹è¯•å®Œæˆ: {len(successful_results)}/{request_count} æˆåŠŸ")
        return metrics

    def save_results(self, metrics_list: List[PerformanceMetrics], output_file: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if output_file is None:
            output_file = f"performance_test_results_{int(time.time())}.json"

        results_data = {
            'timestamp': time.time(),
            'system_info': {
                'platform': sys.platform,
                'python_version': sys.version,
                'cpu_count': psutil.cpu_count(),
                'memory_total_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024
            },
            'test_results': [metric.to_dict() for metric in metrics_list]
        }

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, indent=2, ensure_ascii=False)
            self.logger.info(f"ğŸ“Š æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {e}")

    def print_summary(self, metrics_list: List[PerformanceMetrics]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“Š TTSæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)

        for metric in metrics_list:
            print(f"\nğŸ” {metric.test_name}")
            print(f"   æ€»è¯·æ±‚æ•°: {metric.total_requests}")
            print(f"   æˆåŠŸè¯·æ±‚: {metric.successful_requests}")
            print(f"   å¤±è´¥è¯·æ±‚: {metric.failed_requests}")
            print(f"   é”™è¯¯ç‡: {metric.error_rate:.2%}")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {metric.avg_response_time:.3f}s")
            print(f"   å“åº”æ—¶é—´èŒƒå›´: {metric.min_response_time:.3f}s - {metric.max_response_time:.3f}s")
            print(f"   P95å“åº”æ—¶é—´: {metric.p95_response_time:.3f}s")
            print(f"   P99å“åº”æ—¶é—´: {metric.p99_response_time:.3f}s")
            print(f"   è¯·æ±‚é€Ÿç‡: {metric.requests_per_second:.2f} req/s")
            print(f"   å†…å­˜å¢é•¿: {metric.memory_usage_mb:.2f} MB")
            print(f"   CPUä½¿ç”¨ç‡: {metric.cpu_usage_percent:.1f}%")
            print(f"   å¹³å‡éŸ³é¢‘è´¨é‡: {metric.audio_quality_avg:.1f} åˆ†")

        # æ€§èƒ½è¯„çº§
        print(f"\nğŸ† æ€§èƒ½è¯„çº§:")
        avg_response_time = statistics.mean([m.avg_response_time for m in metrics_list])
        avg_error_rate = statistics.mean([m.error_rate for m in metrics_list])
        avg_memory_growth = statistics.mean([m.memory_usage_mb for m in metrics_list])

        if avg_response_time < 2.0 and avg_error_rate < 0.05 and avg_memory_growth < 50:
            grade = "A+ (ä¼˜ç§€)"
        elif avg_response_time < 3.0 and avg_error_rate < 0.1 and avg_memory_growth < 100:
            grade = "B+ (è‰¯å¥½)"
        elif avg_response_time < 5.0 and avg_error_rate < 0.2:
            grade = "C+ (ä¸€èˆ¬)"
        else:
            grade = "D (éœ€è¦ä¼˜åŒ–)"

        print(f"   ç»¼åˆè¯„çº§: {grade}")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}s")
        print(f"   å¹³å‡é”™è¯¯ç‡: {avg_error_rate:.2%}")
        print(f"   å¹³å‡å†…å­˜å¢é•¿: {avg_memory_growth:.2f} MB")

        print("\n" + "=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    print("XleRobot TTSæ€§èƒ½æµ‹è¯•å·¥å…·")
    print("=" * 50)

    # æ£€æŸ¥ç¯å¢ƒ
    if not os.getenv("ALIBABA_CLOUD_TOKEN"):
        print("âš ï¸  è­¦å‘Š: ALIBABA_CLOUD_TOKEN æœªè®¾ç½®")
        print("   æµ‹è¯•å¯èƒ½ä¼šå¤±è´¥ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡åé‡è¯•")
        return

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = TTSPerformanceTester()

    # åˆå§‹åŒ–ç»„ä»¶
    if not tester.initialize_components():
        print("âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œæ€§èƒ½æµ‹è¯•")
        return

    # æ‰§è¡Œæµ‹è¯•
    metrics_list = []

    try:
        # åŸºç¡€æ€§èƒ½æµ‹è¯•
        metrics = tester.test_basic_performance(request_count=20)
        metrics_list.append(metrics)

        # å¹¶å‘æ€§èƒ½æµ‹è¯•
        metrics = tester.test_concurrent_performance(thread_count=3, requests_per_thread=5)
        metrics_list.append(metrics)

        # å‚æ•°è°ƒèŠ‚æ€§èƒ½æµ‹è¯•
        metrics = tester.test_parameter_performance()
        metrics_list.append(metrics)

        # ç¨³å®šæ€§æµ‹è¯• (ç¼©çŸ­æ—¶é—´ç”¨äºæ¼”ç¤º)
        metrics = tester.test_stability_performance(duration_minutes=1)
        metrics_list.append(metrics)

    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    # ä¿å­˜å’Œæ˜¾ç¤ºç»“æœ
    if metrics_list:
        tester.save_results(metrics_list)
        tester.print_summary(metrics_list)
    else:
        print("âŒ æ²¡æœ‰å®Œæˆä»»ä½•æµ‹è¯•")


if __name__ == "__main__":
    main()